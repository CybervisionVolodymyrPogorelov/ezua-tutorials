{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7wSVnqQ7xZB"
   },
   "source": [
    "# Bike Sharing (MLFlow - KServe)\n",
    "\n",
    "This notebook offers a detailed walkthrough of a comprehensive data science workflow. It covers data preprocessing, model training and evaluation, hyperparameter tuning, experiment tracking with MLFlow, and model deployment using Seldon and KServe. It focuses on the well-known bike sharing dataset from the UCI Machine Learning Repository.\n",
    "\n",
    "![bike-sharing](images/bike-sharing.jpg)\n",
    "(Photo by <a href=\"https://unsplash.com/@zaccastravels?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">ZACHARY STAINES</a> on <a href=\"https://unsplash.com/photos/KEhNcoCldbk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>)\n",
    "\n",
    "The dataset captures the hourly and daily count of rental bikes from 2011 to 2012 in the Capital Bikeshare system. It also includes corresponding weather and seasonal data. The dataset aims to encourage research on bike sharing systems, which are increasingly important for traffic management, environmental sustainability, and public health.\n",
    "\n",
    "The task for this dataset is regression and contains 17,389 instances. The main goal is to build a predictive model that can accurately forecast bike rental demand. The primary target variable for prediction is the `cnt` attribute, which represents the total count of rental bikes, including both casual and registered users.\n",
    "\n",
    "By leveraging additional features in the dataset, such as time and weather information, you will train a model to predict this count with a high level of accuracy.\n",
    "\n",
    "**References:**\n",
    "- https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "- https://docs.databricks.com/_static/notebooks/gbt-regression.html\n",
    "- https://www.kaggle.com/pratsiuk/mlflow-experiment-automation-top-9\n",
    "- https://mlflow.org/docs/latest/tracking.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "The following code cells focus on importing the necessary dependencies. Also, you should set up a local directory to save the training artifacts generated during your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import datetime\n",
    "import subprocess\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from functools import partial\n",
    "\n",
    "import git\n",
    "import boto3\n",
    "import kserve\n",
    "import sklearn\n",
    "import ipywidgets as widgets\n",
    "import s3transfer\n",
    "import requests\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from pydotplus import graph_from_dot_data\n",
    "from IPython.display import Image, display\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client import V1ObjectMeta, V1Secret, V1ServiceAccount, V1ObjectReference, V1Container, V1EnvVar\n",
    "from kserve import (V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1SKLearnSpec,\n",
    "                    V1beta1PredictorSpec, V1beta1ModelSpec, V1beta1ModelFormat)\n",
    "\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"model_artifacts\"):\n",
    "    os.system(\"rm -rf model_artifacts\")\n",
    "os.mkdir(\"model_artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the MLflow experiment\n",
    "\n",
    "First, set up an experiment using MLFlow to track various runs during the model training process. MLFlow is an open-source platform that manages the end-to-end machine learning lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up an MLflow experiment to track model training runs\n",
    "experiment_name = 'bike-sharing-exp'\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwKZC40S-e0R"
   },
   "source": [
    "## Load the Dataset\n",
    "\n",
    "With the preliminary setup complete, you can now move on to loading the dataset. The data comes in CSV format, easily loadable using the Pandas library in Python. To get a quick look at the dataset, you can display the first five rows using the `head()` method of the DataFrame. This initial exploration gives you a snapshot of the data you'll work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "mFGzYdKCCNiK",
    "outputId": "8783bf81-d46a-4958-d2dc-59a324868a64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load input data into pandas dataframe\n",
    "bike_sharing = pd.read_csv(os.path.join(\"data\", \"bike-sharing.csv\"))\n",
    "bike_sharing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQk3RQt2FB8x"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "In this phase, you'll prepare the data for the subsequent stages of your analysis. This involves cleaning, transforming, and structuring the data to make sure it's in the optimal format for your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "vyS5Ru5aE5Y7",
    "outputId": "5d0b2528-9664-437d-8e3f-61119fdaad5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove unused columns\n",
    "bike_sharing.drop(columns=[\"instant\", \"dteday\", \"registered\", \"casual\"], inplace=True)\n",
    "\n",
    "# Use better names\n",
    "bike_sharing.rename(\n",
    "    columns={\n",
    "        \"yr\": \"year\",\n",
    "        \"mnth\": \"month\",\n",
    "        \"hr\": \"hour_of_day\",\n",
    "        \"holiday\": \"is_holiday\",\n",
    "        \"workingday\": \"is_workingday\",\n",
    "        \"weathersit\": \"weather_situation\",\n",
    "        \"temp\": \"temperature\",\n",
    "        \"atemp\": \"feels_like_temperature\",\n",
    "        \"hum\": \"humidity\",\n",
    "        \"cnt\": \"rented_bikes\",\n",
    "    }, inplace=True)\n",
    "\n",
    "# Convert every data point to `float64`\n",
    "cols = bike_sharing.select_dtypes(exclude=['float64']).columns\n",
    "for i in ['season', 'year', 'month', 'hour_of_day', 'is_holiday',\n",
    "          'weekday', 'is_workingday', 'weather_situation', 'rented_bikes']:\n",
    "    bike_sharing[i] = bike_sharing[i].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40MGTHbNFKTP"
   },
   "source": [
    "## Data Visualization\n",
    "\n",
    "In this section, you'll use various visualization techniques to gain a deeper understanding of your data. Creating graphical representations will help you identify patterns, trends, and correlations that might not be obvious in the raw data. This step is crucial for guiding your subsequent analysis and model-building efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "bNZOegwGHzUR",
    "outputId": "45a00d75-c019-4c39-96c4-08e3995fb381",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hour_of_day_agg = bike_sharing.groupby([\"hour_of_day\"])[\"rented_bikes\"].sum()\n",
    "\n",
    "hour_of_day_agg.plot(\n",
    "    kind=\"line\", \n",
    "    title=\"Total rented bikes by hour of day\",\n",
    "    xticks=hour_of_day_agg.index,\n",
    "    figsize=(10, 5),\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMg_JKoUKq9j"
   },
   "source": [
    "## Prepare training and test data sets\n",
    "\n",
    "In this section, you'll partition your data into training and test datasets. This step is crucial in the machine learning workflow. It allows you to train your model on a subset of the data, known as the training set, and then evaluate its performance on unseen data, or the test set. This approach helps ensure that your model generalizes well to new data and avoids simply memorizing the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ZwtDgaZ9Ktie",
    "outputId": "4971f3d6-5e99-4583-acb6-4ef73892a633",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset randomly into 70% for training and 30% for testing.\n",
    "X = bike_sharing.drop(\"rented_bikes\", axis=1)\n",
    "y = bike_sharing.rented_bikes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.size}\")\n",
    "print(f\"Test samples: {X_test.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN0w6zFJSb87"
   },
   "source": [
    "## Establishing Evaluation Metrics\n",
    "\n",
    "Before moving on to the training stage, you'll define the evaluation metrics to assess your model's performance. These metrics offer quantitative measures of the model's accuracy, helping you understand its effectiveness and areas for improvement. This step is crucial for making sure your model meets the desired performance standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC1wzz_T_tSA"
   },
   "source": [
    "### Root Mean Square Error (RMSE)\n",
    "\n",
    "One evaluation metric you'll use is the Root Mean Square Error (RMSE). This metric quantifies the differences between the values your model predicts and the actual values. By taking the square root of the average of these squared differences, RMSE helps you gauge the magnitude of the prediction errors. Lower RMSE values signify a better fit of the model to the data.\n",
    "\n",
    "References: \n",
    "- https://medium.com/@xaviergeerinck/artificial-intelligence-how-to-measure-performance-accuracy-precision-recall-f1-roc-rmse-611d10e4caac\n",
    "- https://www.kaggle.com/residentmario/model-fit-metrics#Root-mean-squared-error-(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhPcLCteQy6j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def rmse_score(y, y_pred):\n",
    "    score = rmse(y, y_pred)\n",
    "    message = \"RMSE score: {:.4f}\".format(score)\n",
    "    return score, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ3nr3D_AE85"
   },
   "source": [
    "### Cross-Validation RMSLE score\n",
    "\n",
    "Another evaluation metric you'll use is the Root Mean Squared Logarithmic Error (RMSLE) score, calculated through cross-validation. Cross-validation is a robust technique that averages measures of prediction accuracy to provide a more accurate estimate of model performance.\n",
    "\n",
    "The RMSLE score is particularly useful in your case because it penalizes underestimates more than overestimates. This makes it an essential metric for your bike sharing demand prediction model, helping you avoid situations where the available number of bikes doesn't meet the demand.\n",
    "\n",
    "References: \n",
    "- https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "- https://www.kaggle.com/carlolepelaars/understanding-the-metric-rmsle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H9CZAP2ASe6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmsle_clip(estimator, x, y):\n",
    "    \"\"\"Clip negative prediction numbers before calculating RMSLE.\"\"\"\n",
    "    y_pred = estimator.predict(x)\n",
    "    y_pred_clipped = np.clip(y_pred, a_min=0, a_max=None)\n",
    "    return sklearn.metrics.mean_squared_log_error(y, y_pred_clipped, squared=False)\n",
    "\n",
    "def rmsle_cv(model, X_train, y_train):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    # Evaluate RMSLE score by cross-validation\n",
    "    rmsle = cross_val_score(model, X_train.values, y_train, scoring=rmsle_clip, cv=kf, error_score=\"raise\")\n",
    "    return rmsle\n",
    "\n",
    "def rmsle_cv_score(model, X_train, y_train):\n",
    "    score = rmsle_cv(model, X_train, y_train)\n",
    "    message = \"Cross-Validation RMSLE score: {:.4f} (std = {:.4f})\".format(score.mean(), score.std())\n",
    "    return score, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad0mABWEarsA"
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "In this section, you'll analyze the importance of each feature in your dataset. Feature importance involves techniques that assign a score to input features based on their usefulness in predicting a target variable.\n",
    "\n",
    "Knowing which features most strongly influence the target variable can give you valuable insights into your dataset and the underlying model. This understanding can help you interpret the model's predictions and guide future data collection and feature engineering efforts.\n",
    "\n",
    "References:\n",
    "- https://medium.com/bigdatarepublic/feature-importance-whats-in-a-name-79532e59eea3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ7kzjbOWae8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_feature_importance(model):\n",
    "    feature_importance = pd.DataFrame(\n",
    "        model.feature_importances_,\n",
    "        index=X_train.columns,\n",
    "        columns=[\"Importance\"])\n",
    "\n",
    "    # sort by importance\n",
    "    feature_importance.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(\n",
    "        data=feature_importance.reset_index(),\n",
    "        y=\"index\",\n",
    "        x=\"Importance\",\n",
    "    ).set_title(\"Feature Importance\")\n",
    "\n",
    "    # save image\n",
    "    plt.savefig(\"model_artifacts/feature_importance.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYfCxPo8w-Gn"
   },
   "source": [
    "## Permutation Importance\n",
    "\n",
    "Permutation Importance is a technique you'll use to measure feature importance. This method randomly shuffles a single feature in the validation data and measures the resulting decrease in the model's performance. Features that lead to the most significant drop in performance are considered the most important.\n",
    "\n",
    "This approach offers a straightforward way to understand the impact of each feature on the model's predictions. It can help you identify which features are driving the model's decisions and where you might concentrate your efforts for further data analysis or feature engineering.\n",
    "\n",
    "References:\n",
    "- https://www.kaggle.com/dansbecker/permutation-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_vzVVbGcS6M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_permutation_importance(model):\n",
    "    p_importance = permutation_importance(model, X_test, y_test, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # sort by importance\n",
    "    sorted_idx = p_importance.importances_mean.argsort()[::-1]\n",
    "    p_importance = pd.DataFrame(\n",
    "        data=p_importance.importances[sorted_idx].T,\n",
    "        columns=X_train.columns[sorted_idx]\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(\n",
    "        data=p_importance,\n",
    "        orient=\"h\"\n",
    "    ).set_title(\"Permutation Importance\")\n",
    "\n",
    "    # save image\n",
    "    plt.savefig(\"model_artifacts/permutation_importance.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtQGsSNU8hWc"
   },
   "source": [
    "## Decision Tree Visualization\n",
    "\n",
    "In this section, you'll visualize the decision tree model. A decision tree resembles a flowchart, where each internal node represents a feature, each branch signifies a decision rule, and each leaf node indicates an outcome.\n",
    "\n",
    "Visualizing the decision tree helps you clearly understand how the model makes predictions. You can see the decision paths and rules the model employs, making it one of the most interpretable machine learning models. This can be especially helpful when you need to explain the model's decisions to stakeholders.\n",
    "\n",
    "References:\n",
    "- https://towardsdatascience.com/visualizing-decision-trees-with-python-scikit-learn-graphviz-matplotlib-1c50b4aa68dc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxKIpaE-g-b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_tree_visualization(model):\n",
    "    # generate visualization\n",
    "    tree_dot_data = tree.export_graphviz(\n",
    "        # Get the first tree\n",
    "        # TODO: Visualize every tree\n",
    "        decision_tree=model.estimators_[0, 0],\n",
    "        label=\"all\",\n",
    "        feature_names=X_train.columns,\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        proportion=True,\n",
    "        impurity=False,\n",
    "        precision=1,\n",
    "    )\n",
    "\n",
    "    # save image\n",
    "    graph_from_dot_data(tree_dot_data).write_png(\"model_artifacts/Decision_Tree_Visualization.png\")\n",
    "\n",
    "    # show tree\n",
    "    return graphviz.Source(tree_dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "warpAv8RFSOI"
   },
   "source": [
    "## MLflow Tracking\n",
    "\n",
    "In this phase, you'll use MLflow Tracking, a component of MLflow designed to log and track experiment data. This data includes parameters, metrics, and artifacts generated during the model training process.\n",
    "\n",
    "MLflow Tracking offers a centralized repository for metadata related to your experiments. This makes it easier to compare different runs, reproduce results, and share findings with your team. This step is vital for maintaining an organized and efficient machine learning workflow.\n",
    "\n",
    "First, let's set up the logger.\n",
    "\n",
    "References:\n",
    "- https://www.mlflow.org/docs/latest/cli.html#mlflow-ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyQRcKslAwv-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track params and metrics\n",
    "def log_mlflow_run(model, signature):\n",
    "    # Auto-logging for scikit-learn estimators\n",
    "    # mlflow.sklearn.autolog()\n",
    "\n",
    "    # log estimator_name name\n",
    "    name = model.__class__.__name__\n",
    "    mlflow.set_tag(\"estimator_name\", name)\n",
    "\n",
    "    # log input features\n",
    "    mlflow.set_tag(\"features\", str(X_train.columns.values.tolist()))\n",
    "\n",
    "    # Log tracked parameters only\n",
    "    mlflow.log_params({key: model.get_params()[key] for key in parameters})\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        'RMSLE_CV': score_cv.mean(),\n",
    "        'RMSE': score})\n",
    "\n",
    "    # log training loss\n",
    "    for s in model.train_score_:\n",
    "        mlflow.log_metric(\"Train Loss\", s)\n",
    "\n",
    "    # Save model to artifacts\n",
    "    mlflow.sklearn.log_model(model, \"model\")#, signature=signature)\n",
    "\n",
    "    # log charts\n",
    "    mlflow.log_artifacts(\"model_artifacts\")\n",
    "\n",
    "    # misc\n",
    "    # Log all model parameters\n",
    "    # mlflow.log_params(model.get_params())\n",
    "    mlflow.log_param(\"Training size\", X_test.size) \n",
    "    mlflow.log_param(\"Test size\", y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDAdPTeDTjr1"
   },
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "\n",
    "In this section, you'll focus on training your model and tuning its hyperparameters. For this specific use case, you'll use the following approach:\n",
    "\n",
    "- Approach: You'll employ a Supervised Learning method, specifically a Decision Tree model. Decision Trees are intuitive and easy-to-understand models that make decisions based on a set of rules derived from the features.\n",
    "- Tree Type: Since your task involves predicting a continuous target variable—the total count of rental bikes—you'll use a Regression Tree. Regression Trees predict outcomes by learning simple decision rules based on the features.\n",
    "- Technique/Ensemble Method: To enhance the performance of your Decision Tree model, you'll use an ensemble method called Gradient Boosting. This technique combines several weak learners—in this case, Decision Trees—to create a robust predictive model. It trains models in a gradual, additive, and sequential manner, with each new model correcting the errors of the previous ones.\n",
    "\n",
    "By carefully tuning the hyperparameters of your Gradient Boosting model, you can optimize its performance and ensure it generalizes well to new data.\n",
    "\n",
    "References:\n",
    "- GBRT (Gradient Boosted Regression Tree): https://orbi.uliege.be/bitstream/2268/163521/1/slides.pdf\n",
    "- Choosing a model: https://scikit-learn.org/stable/tutorial/machine_learning_map\n",
    "- Machine Learning Models Explained\n",
    ": https://docs.paperspace.com/machine-learning/wiki/machine-learning-models-explained\n",
    "- Gradient Boosted Regression Trees: https://orbi.uliege.be/bitstream/2268/163521/1/slides.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSbcPvkBThXV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GBRT (Gradient Boosted Regression Tree) scikit-learn implementation \n",
    "model_class = GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7BYFTSRzLk2"
   },
   "source": [
    "Set the training's process hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Mu88JOkMiJF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": [0.1, 0.05, 0.01],\n",
    "    \"max_depth\": [4, 5, 6],\n",
    "    # \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnUDX2p2j9p_",
    "tags": []
   },
   "source": [
    "To optimize your model's performance, you'll tune its hyperparameters using Grid Search.\n",
    "\n",
    "Grid Search is a traditional method for hyperparameter tuning that defines a grid of hyperparameters and evaluates the model's performance at each grid point. You can think of this as an exhaustive search through a manually specified subset of the hyperparameter space for the chosen algorithm.\n",
    "\n",
    "By using Grid Search, you can systematically explore multiple combinations of hyperparameters to find the optimal values that improve your model's performance. This process can significantly enhance the predictive accuracy of your model.\n",
    "\n",
    "References:\n",
    "- More advanced tuning techniques: https://research.fb.com/efficient-tuning-of-online-systems-using-bayesian-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CybsVlgCw6n9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate parameters combinations\n",
    "params_keys = parameters.keys()\n",
    "params_values = [\n",
    "    parameters[key] if isinstance(parameters[key], list) else [parameters[key]]\n",
    "    for key in params_keys]\n",
    "\n",
    "runs_parameters = [\n",
    "    dict(zip(params_keys, combination)) for combination in itertools.product(*params_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23-Tpn_0X7d"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "Now that you've prepared your data and set up your model, the next step is to train the model. During this process, the model will learn from the features in your training data to predict the target variable.\n",
    "\n",
    "Training the model involves adjusting it to minimize the difference between the predicted and actual values, guided by a specific learning algorithm. In your case, you're using a Gradient Boosting model, which will learn to correct its errors in a gradual, additive, and sequential manner.\n",
    "\n",
    "This step is crucial in your machine learning workflow, as the quality of your model's predictions largely depends on the effectiveness of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "Le6sa7jjg37v",
    "outputId": "7e8c3e45-e75a-45ce-8157-38b097d623cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(runs_parameters),\n",
    "    # description='Training progress',\n",
    "    bar_style='info',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "progress_text = widgets.Output()\n",
    "\n",
    "display(progress_bar, progress_text)\n",
    "\n",
    "# training loop\n",
    "for i, run_parameters in enumerate(runs_parameters):\n",
    "    progress_bar.description = f\"Run {i+1}/{len(runs_parameters)}\"\n",
    "    # mlflow: stop active runs if any\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "    # mlflow:track run\n",
    "    mlflow.start_run(run_name=f\"Run {i}\")\n",
    "\n",
    "    # create model instance\n",
    "    model = model_class(**run_parameters)\n",
    "\n",
    "    # train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # get evaluations scores\n",
    "    ypred = model.predict(X_test)\n",
    "    score, message = rmse_score(y_test, model.predict(X_test))\n",
    "    score_cv, message_cv = rmsle_cv_score(model, X_train, y_train)\n",
    "    \n",
    "    # generate charts\n",
    "    model_feature_importance(model)\n",
    "    plt.close()\n",
    "    model_permutation_importance(model)\n",
    "    plt.close()\n",
    "    # model_tree_visualization(model)\n",
    "\n",
    "    # get model signature\n",
    "    signature = infer_signature(model_input=X_train, model_output=model.predict(X_train))\n",
    "\n",
    "    # mlflow: log metrics\n",
    "    log_mlflow_run(model, signature)\n",
    "\n",
    "    # mlflow: end tracking\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    progress_bar.value = i + 1\n",
    "    \n",
    "    # Update progress text\n",
    "    with progress_text:\n",
    "        print(f\"Learning rate: {run_parameters['learning_rate']}\"\n",
    "              f\"\\tMax depth: {run_parameters['max_depth']}\")\n",
    "        print(message)\n",
    "        print(message_cv)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOHX6U3ABTSE"
   },
   "source": [
    "## Best Model Results\n",
    "\n",
    "After training several models and tuning their hyperparameters, you'll identify the model that performs best according to your chosen evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5jKy850zKtS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_run_df = mlflow.search_runs(order_by=['metrics.RMSLE_CV ASC'], max_results=1)\n",
    "if len(best_run_df.index) == 0:\n",
    "    raise Exception(f\"Found no runs for experiment '{experiment_name}'\")\n",
    "\n",
    "best_run = mlflow.get_run(best_run_df.at[0, 'run_id'])\n",
    "best_model_uri = f\"{best_run.info.artifact_uri}/model\"\n",
    "best_model = mlflow.sklearn.load_model(best_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "wHVM74A--4-C",
    "outputId": "b28470ff-aa99-4f19-c124-d4b9c3a88233",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print best run info\n",
    "print(\"Best run info:\")\n",
    "print(f\"Run id: {best_run.info.run_id}\")\n",
    "print(f\"Run parameters: {best_run.data.params}\")\n",
    "print(f\"Run score: RMSLE_CV = {best_run.data.metrics['RMSLE_CV']:.4f}\")\n",
    "print(f\"Run model URI: {best_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "WmjSO3vhCP7u",
    "outputId": "52e2c4ac-4aeb-44b8-d65d-7d2de8122fb8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_feature_importance(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "LQRJKFuJCSBZ",
    "outputId": "3a60cd14-f402-4304-ee8c-7f3647be4721",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_permutation_importance(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "fR2F0ex7CS4I",
    "outputId": "ff25041b-c91e-4372-ab49-514e350bc709",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tree_visualization(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDhu91aa8vuw"
   },
   "source": [
    "## Model Testing\n",
    "\n",
    "Once you've identified your best model, the next step is to test its predictive performance on unseen data using the test dataset, which you set aside specifically for this purpose.\n",
    "\n",
    "Testing the model's predictions allows you to assess how well the model generalizes to new data. This step is crucial in the machine learning process because it provides a realistic estimate of the model's performance in real-world settings.\n",
    "\n",
    "You'll compare the model's predictions with the actual values in the test dataset and calculate your chosen evaluation metrics. These results will give you a clear indication of the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "EiQwrb7TK40n",
    "outputId": "709d749f-bc0d-4b68-c2c4-8c1a0197eca6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = X_test.copy()\n",
    "# real output (rented_bikes) from test dataset\n",
    "test_predictions[\"rented_bikes\"] = y_test\n",
    "\n",
    "# add \"predicted_rented_bikes\" from test dataset\n",
    "test_predictions[\"predicted_rented_bikes\"] = best_model.predict(X_test).astype(int)\n",
    "\n",
    "# show results\n",
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "id": "SwfQEr_NGlDa",
    "outputId": "e153d67b-b13f-4b13-bbe9-542eed7442a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot truth vs prediction values\n",
    "test_predictions.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"rented_bikes\",\n",
    "    y=\"predicted_rented_bikes\",\n",
    "    title=\"Rented bikes vs predicted rented bikes\",\n",
    "    figsize=(10, 10)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "In this section of the notebook, you'll focus on deploying the trained model to bridge the gap between insightful data analysis and real-world impact. To do this, you'll use KServe, an open-source platform that simplifies the deployment and management of machine learning models at scale. KServe offers a robust and scalable infrastructure for serving predictions from trained models in production environments. For KServe's backend, you'll use Seldon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isvc = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: kserve-minio-secret\n",
    "secrets:\n",
    "- name: {0}-objectstore-secret\n",
    "\n",
    "---\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: bike-sharing-exp\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-minio-secret\n",
    "    sklearn:\n",
    "      protocolVersion: v2\n",
    "      storageUri: {1}\n",
    "\"\"\".format(os.environ['USER'], best_model_uri)\n",
    "\n",
    "with open(os.path.join(\"manifests\", \"isvc.yaml\"), \"w\") as f:\n",
    "    f.write(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"manifests/isvc.yaml\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMUyEIXKPIvKiU5I2T//pwx",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MLflow-example-notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-bike-sharing]",
   "language": "python",
   "name": "conda-env-.conda-bike-sharing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
