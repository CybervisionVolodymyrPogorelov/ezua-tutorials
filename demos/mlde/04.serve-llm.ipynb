{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Large Language Model Inference Service\n",
    "\n",
    "Welcome to the fourth part of the tutorial series on building a question-answering application over a corpus of private\n",
    "documents using Large Language Models (LLMs). In the previous Notebooks, you journeyed through the processes of creating\n",
    "vector embeddings of our documents, setting up the Inference Services (ISVCs) for the Vector Store and the Embeddings model,\n",
    "and testing the performanc of the information retrieval system the two components define.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/llm.jpg\" alt=\"llm\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Google DeepMind</a> on <a href=\"https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Now, you're moving towards the next crucial step: creating an ISVC for the LLM. This ISVC is the centerpiece of the\n",
    "question-answering system, working in tandem with the Vector Store ISVC to deliver comprehensive and accurate answers to\n",
    "user queries.\n",
    "\n",
    "In this Notebook, you set up this LLM ISVC. You learn how to build a Docker image for the custom transformer and its\n",
    "role define a KServe ISVC YAML file, and deploy the service. By the end of this Notebook, you'll have a fully functioning\n",
    "LLM ISVC that can accept user queries, interact with the Vector Store, and provide insightful responses.\n",
    "\n",
    "To complete this step you need to have access to a PVC named `llama-pvc`, which contains the TensorRT-LLM engines for the\n",
    "Llamma 2 7B model, and the Hugging Face repository of the actual model:\n",
    "\n",
    "* The TensorRT-LLM engines can be downloaded from the following link: https://ezmeral-artifacts.s3.us-east-2.amazonaws.com/llama-engines.tar.gz.\n",
    "  After downloading, please extract the contents of this tarball into the llama-pvc volume.\n",
    "* The model repository is available via the Hugging Face hub at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
    "  It is not necessary to clone the model weights, thus eliminating the need for Git LFS.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Architecture](#architecture)\n",
    "1. [Creating the Inference Service](#creating-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0cc08-1c00-49cc-85bd-b792383b26e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Architecture\n",
    "\n",
    "In this setup, an additional component, called a \"transformer\", plays a pivotal role in processing user queries and\n",
    "integrating the Vector Store ISVC with the LLM ISVC. The transformer's role is to intercept the user's request, extract\n",
    "the necessary information, and then communicate with the Vector Store ISVC to retrieve the relevant context. The\n",
    "transformer then takes the response of the Vector Store ISVC (i.e., the context), combines it with the user's query, and\n",
    "forwards the enriched prompt to the LLM predictor.\n",
    "\n",
    "Here's a detailed look at the process:\n",
    "\n",
    "1. **Intercepting the User's Request**: The transformer acts as a gateway between the user and the LLM ISVC. When a user\n",
    "   sends a query, it first reaches the transformer. The transformer extracts the query from the request.\n",
    "1. **Communicating with the Vector Store ISVC**: The transformer then takes the user's query and sends a POST request to the\n",
    "   Vector Store ISVC including the user's query in the payload, just like you did in the previous Notebook.\n",
    "1. **Receiving and Processing the Context**: The Vector Store ISVC responds by sending back the relevant context.\n",
    "1. **Combining the Context with the User's Query**: The transformer then combines the received context with the user's\n",
    "   original query using a prompt template. This creates an enriched prompt that contains both the user's original\n",
    "   question and the relevant context from our documents.\n",
    "1. **Forwarding the Enriched Query to the LLM Predictor**: Finally, the transformer forwards this enriched query to the LLM\n",
    "   predictor. The predictor then processes this query and generates a response, which is sent back to the transformer.\n",
    "   Steps 2 through 5 are transparent to the user.\n",
    "1. **Final response**:The transformer returns the response to the user.\n",
    "\n",
    "As such, you should build one custom Docker image at this point for the transformer component. The\n",
    "source code and the Dockerfile is provided in the corresponding folder: `dockerfiles/transformer`.\n",
    "For your convenience, you can use the image we have pre-built for you: `dpoulopoulos/qna-transformer-mlde:v0.1.0`\n",
    "\n",
    "Once ready, proceed with the next steps.\n",
    "\n",
    "# Creating the Inference Service\n",
    "\n",
    "As before, you need to provide the name of the transofmer image You can leave any field empty to use the image we\n",
    "provide for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db302a3-e35f-4bb3-a2c5-24ee637c2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Transformer Image</h2>\")\n",
    "display(heading)\n",
    "\n",
    "transformer_image_widget = widgets.Text(\n",
    "    description=\"Image Name:\",\n",
    "    placeholder=\"Default: dpoulopoulos/qna-transformer-mlde:v0.1.0\",\n",
    "    layout=widgets.Layout(width='30%'))\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "success_message = widgets.Output()\n",
    "\n",
    "transformer_image = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global transformer_image\n",
    "    transformer_image = transformer_image_widget.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        if not transformer_image:\n",
    "            transformer_image = \"dpoulopoulos/qna-transformer-mlde:v0.1.0\"\n",
    "        print(f\"The name of the transformer image will be: '{transformer_image}'\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(transformer_image_widget, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c9d-72e4-491a-ad99-f6f533f0ef94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_isvc = \"\"\"\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"ensemble\"\n",
    "spec:\n",
    "  predictor:\n",
    "    volumes:\n",
    "    - name: llama-volume\n",
    "      persistentVolumeClaim:\n",
    "        claimName: llama-pvc\n",
    "    timeout: 600\n",
    "    triton:\n",
    "      image: nvcr.io/nvidia/tritonserver:24.01-trtllm-python-py3\n",
    "      securityContext:\n",
    "          runAsUser: 0\n",
    "      volumeMounts:\n",
    "      - mountPath: \"/mnt/llama\"\n",
    "        name: llama-volume\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: \"4\"\n",
    "          memory: 32Gi\n",
    "          nvidia.com/gpu: 1\n",
    "        requests:\n",
    "          cpu: \"4\"\n",
    "          memory: 32Gi\n",
    "      storageUri: \"pvc://kubeflow-shared-pvc/inflight_batcher_llm\"\n",
    "  transformer:\n",
    "    affinity:\n",
    "      nodeAffinity:\n",
    "        requiredDuringSchedulingIgnoredDuringExecution:\n",
    "          nodeSelectorTerms:\n",
    "          - matchExpressions:\n",
    "            - key: \"nvidia.com/gpu.present\"\n",
    "              operator: NotIn\n",
    "              values:\n",
    "              - \"true\"\n",
    "    timeout: 600\n",
    "    containers:\n",
    "      - image: {0}\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        name: kserve-container\n",
    "        args: [\"--protocol\", \"v2\"]\n",
    "\"\"\".format(transformer_image)\n",
    "\n",
    "with open(\"llama-isvc.yaml\", \"w\") as f:\n",
    "    f.write(llama_isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631d06-ea76-4159-a08b-57e76850ff56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"llama-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully built an LLM ISVC, and\n",
    "you've learned about the role of a transformer in enriching user queries with relevant context from our documents.\n",
    "Together with the Vector Store ISVC, these components form the backbone of your question-answering application.\n",
    "\n",
    "However, the journey doesn't stop here. The next and final step is to test the LLM ISVC, ensuring that it's working as\n",
    "expected and delivering accurate responses. This will help you gain confidence in your setup and prepare you for\n",
    "real-world applications. In the next Notebook, you invoke the LLM ISVC. You see how to construct suitable requests,\n",
    "communicate with the service, and interpret the responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
