# MLDE Demo (WiP)

In this tutorial, you build a question-answering RAG (Retrieval Augmented Generation )system using
an open source Large Language Model (LLM) and a fine-tuned embeddings model. This system can answer
questions from a corpus of private documentation.

To make this happen, you:

* Deploy an embeddings model, fine-tuned on a corpus of private documentation.
* Use this model to build and deploy a Vector Store, designed to capture and index the latent
  representation of each document effectively.
* Deploy an LLM to respond to inquiries, leveraging the context extracted from the Vector Store, in
  a natural language format.

1. [What You'll Need](#what-youll-need)
1. [Procedure](#procedure)
1. [Troubleshooting](#troubleshooting)
1. [How It Works](#how-it-works)
1. [Clean Up](#clean-up)
1. [References](#references)

## What You'll Need

For this tutorial, ensure you have:

- Access to an HPE Ezmeral Unified Analytics (EzUA) cluster.

## Procedure

## Troubleshooting

## How It Works

## Clean Up

## References
