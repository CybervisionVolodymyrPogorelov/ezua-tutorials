{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c591b2e4-69bf-4932-a330-4e43632e2d63",
   "metadata": {},
   "source": [
    "founder: Dirk Derichsweiler, Contributors: Vincent Charbonnier and Isabelle Steinhauser, September 2023\n",
    "# Data Streaming Infrastructure | Consumer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0241829-1131-4953-a5fb-ac59f3a27ed7",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./media/data_streaming.gif\" style=\"height:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960f8bf-d0da-4551-9d64-32296da7e979",
   "metadata": {},
   "source": [
    "The Streaming Part of the demo shows retail stores publishing data to Data Fabric streams and then this data is trasnferred to EZUA. Refer to Notebook _1-Producer.ipynb_ to follow how the data is published. In this notebook is explained how the data is transferred to UA as shown in below picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1aa10-5e38-4bc0-949e-5ed1fd4a3e9d",
   "metadata": {},
   "source": [
    "# transfer data to Ezmeral Unified Analytics infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd2e25-f4f7-4cb6-bba4-1a1ad330aebb",
   "metadata": {},
   "source": [
    "<img src=\"./media/data_streaming_consumer.gif\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c97ba-e0e0-4460-9fe1-d22120fc753c",
   "metadata": {},
   "source": [
    "In order to transfer the data to EZUA there are 2 steps needed:\n",
    "    \n",
    "    1. get the data out of the DF stream\n",
    "    2. get the data into EZUA\n",
    "    \n",
    "In order to achieve __1__ we're using a __Kafka Consumer__ to connect to the DF and consume the messages in the DF like this:\n",
    "```python\n",
    "consumer = KafkaConsumer(input_topic, bootstrap_servers=kafka_servers,\n",
    "                         security_protocol='SASL_PLAINTEXT',\n",
    "                         sasl_mechanism='PLAIN',\n",
    "                         sasl_plain_username=user,\n",
    "                         sasl_plain_password=pw)\n",
    "```\n",
    "These messages are written into a CSV file (function _consume_and_write_csv_).\n",
    "  \n",
    "  \n",
    "To achieve __2__ we are saving the CSV file in which we stored all the messages into S3 inside EZUA (function _upload_csv_to_s3_).\n",
    "You can find the whole script in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c32d7-9bf9-46cf-a16e-ddcbb4aa509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer  # Import KafkaConsumer to interact with Kafka\n",
    "import csv  # Import csv module for reading and writing CSV files\n",
    "import boto3  # Import boto3 library for interacting with AWS services\n",
    "import urllib3  # Import urllib3 library for disabling warnings\n",
    "import os  # Import os module for file operations\n",
    "\n",
    "urllib3.disable_warnings()  # Disable warnings from urllib3 library\n",
    "\n",
    "# Define a function to consume messages from Kafka and write to CSV file\n",
    "def consume_and_write_csv(consumer, output_file, completion_keyword):\n",
    "    with open(output_file, 'w', newline='') as file:  # Open output file in write mode\n",
    "        csv_writer = csv.writer(file)  # Create a CSV writer object\n",
    "\n",
    "        for message in consumer:  # Iterate over messages received from Kafka\n",
    "            row = message.value.decode('utf-8')  # Decode the message value from bytes to string\n",
    "\n",
    "            if row == completion_keyword:  # Check if the message indicates completion\n",
    "                break  # Exit the loop if completion keyword is received\n",
    "\n",
    "            fields = row.split(',')  # Split the row string into fields\n",
    "            csv_writer.writerow(fields)  # Write the fields as a row in the CSV file\n",
    "\n",
    "# Define a function to upload CSV file to S3 bucket\n",
    "def upload_csv_to_s3(input_file, bucket_name, object_key, username, password, endpoint):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=username,\n",
    "        aws_secret_access_key=password\n",
    "    )\n",
    "    s3 = session.client('s3', endpoint_url=f\"{endpoint}:{port}\", verify=False)  # Create an S3 client\n",
    "\n",
    "    with open(input_file, 'rb') as file:  # Open the input file in binary read mode\n",
    "        s3.upload_fileobj(file, bucket_name, object_key)  # Upload the file to S3 bucket\n",
    "\n",
    "    # Delete the file after successful transfer\n",
    "    # os.remove(input_file)\n",
    "\n",
    "# Configuration parameters\n",
    "# Configuration\n",
    "output_file = 'output.csv'  # Path to the output CSV file\n",
    "input_topic = 'demo'  # Kafka topic to consume messages from\n",
    "kafka_servers = '10.1.84.129:9092,10.1.84.130:9092'  # Kafka server address and port\n",
    "completion_keyword = 'endofdemo'  # Keyword to mark the end of data stream\n",
    "bucket_name = 'ezaf-demo'  # Name of the S3 bucket\n",
    "object_key = 'output.csv'  # Key of the object in S3 bucket\n",
    "endpoint = 'https://home.ezua-cb.ezmeral.demo.local'  # S3-compatible endpoint\n",
    "port = '31900'  # Port for the S3-compatible endpoint\n",
    "aws_access_key_id = 'minioadmin'  # AWS access key ID for authentication\n",
    "aws_secret_key = 'minioadmin'  # AWS secret key for authentication\n",
    "user = 'mapr'\n",
    "pw = 'mapr123'\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = KafkaConsumer(input_topic, bootstrap_servers=kafka_servers,\n",
    "                         security_protocol='SASL_PLAINTEXT',\n",
    "                         sasl_mechanism='PLAIN',\n",
    "                         sasl_plain_username=user,\n",
    "                         sasl_plain_password=pw)\n",
    "\n",
    "# Consume messages and write to CSV file\n",
    "consume_and_write_csv(consumer, output_file, completion_keyword)\n",
    "\n",
    "# Close the consumer\n",
    "consumer.close()\n",
    "\n",
    "# Move the file to S3 bucket\n",
    "upload_csv_to_s3(output_file, bucket_name, object_key, aws_access_key_id, aws_secret_key, endpoint)\n",
    "\n",
    "print(\"File transferred to \" + endpoint + \":\" + port + \" successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc206032-e887-486c-af56-8435ae79c3bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8b974-6e7e-47e3-96a5-cdccaf846928",
   "metadata": {},
   "outputs": [],
   "source": [
    "python ../create_load_transform_data/import_data.py -db mysql -H ddk3s.westcentralus.cloudapp.azure.com -u root -p nfWCEHWNDe -P 31870 -d db_g1 -t sales_data -c \"./data/Czech Republic_sales_data_2019_2023.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fe678-59a7-4251-a6ef-6a293319fa4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "137f53d2-13d2-4dcd-af69-4deee96400d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"1024\" height=\"768\" controls>\n",
       "  <source src=\"../videos/1-faster-no-sound.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"1024\" height=\"768\" controls>\n",
    "  <source src=\"../videos/1-faster-no-sound.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-full:ezaf-fy23-q2",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-external-df-volume:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
