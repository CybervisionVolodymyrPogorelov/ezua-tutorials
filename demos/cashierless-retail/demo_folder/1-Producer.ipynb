{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d88fdd3-ef6f-4aed-bb2a-e21b2f3056ee",
   "metadata": {},
   "source": [
    "# Data Streaming Infrastructure | Consumer\n",
    "created by Dirk Derichsweiler November 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242716d-e6ab-47ab-845a-b93f447e6357",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./media/data_streaming.gif\" style=\"height:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91480ed1-1539-4ad9-b9cf-d6d7f7cb8210",
   "metadata": {},
   "source": [
    "The Streaming Part of the demo shows retail stores publishing data to Data Fabric streams and then this data is transfered to EZUA. Refer to Notebook 1-Consumer.ipynb to follow how the data is transferred to EZUA. In this notebook is explained how the data is published to DF as shown in below picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062539ee-6b4d-4799-9ab1-1d01255ca396",
   "metadata": {},
   "source": [
    "<img src=\"./media/data_streaming_producer.gif\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a31100-bced-41ae-9d2a-8cac6ad4b14e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# create a Dataset and store it under folder data\n",
    "\n",
    "First we need to create data which we then can later produce into DF. We are creating data for three different countries: Czech Republic, Germany, and Switzerland.\n",
    "    \n",
    "<img src=\"./media/czech.jpg\" style=\"height:50px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656c469-4bbe-4fd4-beac-66aeead1d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../create_load_transform_data/create_csv.py -csv ./czech_sales_data_2019_2023.csv -c 'Czech Republic' -cu CZK -s 5 -sy 2019 -ey 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38408ff4-c5be-478f-8b90-b7b6cbfcdfb6",
   "metadata": {},
   "source": [
    "<img src=\"./media/germany.jpg\" style=\"height:50px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb7ad9-5873-4647-86db-7ee8d53dd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../create_load_transform_data/create_csv.py -csv ./czech_sales_data_2019_2023.csv -c 'Germany' -cu EUR -s 15 -sy 2019 -ey 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098c237-ed21-49ad-8954-3ed92fc69283",
   "metadata": {},
   "source": [
    "<img src=\"./media/swiss.jpg\" style=\"height:50px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ed975-c972-4cb1-b735-52a90456f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../create_load_transform_data/create_csv.py -csv ./czech_sales_data_2019_2023.csv -c 'Swiss' -cu CHF -s 5 -sy 2019 -ey 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53a213-fe73-4088-a428-63da5bf11e06",
   "metadata": {},
   "source": [
    "After we created this data now we want to publish the data into a Topic inside DF. We're using the topic _demo_ which we created in advance via the DF GUI. We're writing row by row into the Topic in the function copy_csv_file using a KafkaProducer.\n",
    "```python\n",
    "    producer = KafkaProducer(bootstrap_servers=kafka_servers,\n",
    "                             security_protocol='SASL_PLAINTEXT',\n",
    "                             sasl_mechanism='PLAIN',\n",
    "                             sasl_plain_username=user,\n",
    "                             sasl_plain_password=pw)\n",
    "```\n",
    "    \n",
    "With this we are using one of the several Data Fabric capabilities which is Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff4df2-263c-4afa-8133-55e331239cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import csv\n",
    "\n",
    "def copy_csv_file(input_file, output_topic, kafka_servers, completion_keyword):\n",
    "    producer = KafkaProducer(bootstrap_servers=kafka_servers,\n",
    "                             security_protocol='SASL_PLAINTEXT',\n",
    "                             sasl_mechanism='PLAIN',\n",
    "                             sasl_plain_username=user,\n",
    "                             sasl_plain_password=pw)\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        ## Skip header if present\n",
    "        #header = next(csv_reader, None)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            # Convert row to string\n",
    "            message = ','.join(row).encode('utf-8')\n",
    "            # Publish the row as a message to the Kafka topic\n",
    "            producer.send(output_topic, value=message)\n",
    "\n",
    "        # Publish completion keyword\n",
    "        producer.send(output_topic, value=completion_keyword.encode('utf-8'))\n",
    "\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "# Configuration\n",
    "input_file = 'data/Germany_sales_data_2019_2023.csv'\n",
    "user = 'mapr'\n",
    "pw = 'mapr123'\n",
    "output_topic = 'demo'\n",
    "kafka_servers = '10.1.84.129:9092,10.1.84.130:9092'\n",
    "completion_keyword = 'endofdemo'\n",
    "\n",
    "# Copy CSV file to Kafka topic\n",
    "copy_csv_file(input_file, output_topic, kafka_servers, completion_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db39fbd-67a8-40bf-9209-604114586cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-full:ezaf-fy23-q2",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-external-df-volume:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
