{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86bac2dc-3599-441e-ab23-d952d84d1e24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 1: ETL with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed95f26-3b1a-4ead-80e9-404ad70171bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What we will cover:\n",
    "- Update Group Parameters\n",
    "- Extract data from SQL\n",
    "- Transform the Currencies to Euros\n",
    "- Load Data to Delta tables\n",
    "- Time Travel with Delta Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34b061-c46d-4864-ad93-36367fd76695",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> Make sure you chose PySpark for your kernel\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce431084-dbf6-46c0-9378-d31139a83d2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Architecture\n",
    "![image.png](./images/exercise1/spark_archi.PNG)\n",
    "\n",
    "## Apache Spark Architecture: Driver and Executors\n",
    "\n",
    "### Cluster Manager:\n",
    "- Manages resources across the cluster, allocating Executors to Spark applications.\n",
    "- Examples include Apache Mesos, Apache Hadoop YARN, and Spark's standalone cluster manager.\n",
    "- Responsible for monitoring and scheduling tasks on worker nodes.\n",
    "\n",
    "### Driver:\n",
    "- Centralized control unit of a Spark application.\n",
    "- Runs the main() function and creates a SparkContext.\n",
    "- Manages the execution of the Spark program.\n",
    "- Coordinates tasks and schedules their execution on Executors.\n",
    "- Keeps track of the overall application state.\n",
    "- Communicates with the cluster manager to acquire and allocate resources.\n",
    "\n",
    "### Executors:\n",
    "- Worker nodes that perform the actual data processing tasks.\n",
    "- Assigned tasks by the Driver for parallel execution.\n",
    "- Responsible for running the individual stages of a Spark application.\n",
    "- Manage and cache data in-memory during computation for efficient processing.\n",
    "- Communicate with each other and the Driver for task coordination.\n",
    "- Executors are launched on worker nodes by the cluster manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be59379-6d08-426b-b759-80942d845b76",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Setup Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e337-f157-4cc2-9160-6f7cbaa69aaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### We will use predefined commands for this Notebook called magic command to setup the Spark environment as below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c151a6ec-2416-4175-8bde-827def318778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41f89ca9bb748e29b39747aadc44c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='No sessions yet.'))), CreateSessi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e235c95a7e14c4ebb81da5c89396e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [],
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a51605-5479-42f6-8213-aad197909917",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Run the `%manage_spark` command.\n",
    "1. Click the `Add Endpoint` tab.\n",
    "1. Leave the settings as-is and click on `Add Endpoint` on the right.\n",
    "\n",
    "1. Click on `Create Session` tab.\n",
    "1. Your endpoint should appear in `Endpoint`\n",
    "1. Update the `Name` to your group name\n",
    "1. Change the `Language` to `Python`\n",
    "1. Clic on `Create Session` on the right side\n",
    "\n",
    "<!-- ![image.png](./images/exercise1/init_spark2.PNG) -->\n",
    "\n",
    "<!-- ![image.png](./images/exercise1/init_spark.PNG) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021cd64-98ba-4810-8882-9205933056f8",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beaf2946-32f2-40b6-88cc-0af78fed48f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Spark session will take a few minutes to start.\n",
    "\n",
    "We will check the status of the session in the interim: \n",
    "\n",
    "1. Navigate back to the Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu, select `Spark Interactive Sessions`.\n",
    "\n",
    "![image.png](./images/exercise1/menu.PNG)\n",
    "\n",
    "3. Here, you can check the status of your session. It will take 2-3 minutes to start. When the `State` says `Idle`, the session is ready. \n",
    "\n",
    "![image.png](./images/exercise1/session.PNG)\n",
    "\n",
    "4. Scroll back up to the Notebook cell of the session (%manage_spark command). Confirm under the `Manage Sessions` tab that the session should now be visible as `Idle` too. \n",
    "\n",
    "![image.png](./images/exercise1/session2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43012d3a-51e1-40ba-8827-c1ef21175533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89506a-93b1-4673-8064-3f51470ce265",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> Do not restart the kernel\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b65af-8885-4716-8704-a56ddf4f3b34",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Run the `%config_spark` magic command.\n",
    "2. you want a specific configuration, leave the settings as-is and clic on `Submit` at the bottom\n",
    "\n",
    "![image.png](./images/exercise1/config.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a6e1-bee4-4d43-aa24-c0e429963489",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up the group parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f5880-a82d-47b2-a630-f4401a0ec665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your group name\n",
    "group_name = \"user1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79c95e-d669-4575-adba-c2ce7553a120",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d03e4-d6ea-4a42-85cf-8f75731209d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68571ddd-0bbb-4112-8d86-f431d3b5d0be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up the Spark session\n",
    "Adding delta extensions to the configuration to be able to interact with the delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6811656-d54e-466e-89a1-f7e9719d4b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCleaningWithSpark\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Pyspark session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a7dbb-003b-424d-bb37-302ce6493c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Extract Data from SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563439dc-aad6-4055-a8a6-8324373c00ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set connection parameters \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> Make sure the values are correct\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca4040-ea0b-4ed6-85c8-43b67d3200ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the JDBC connection properties\n",
    "mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Hpepoc@123\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "}\n",
    "\n",
    "def get_jdbc_url_for_country(country):\n",
    "    # Define JDBC URLs for different countries\n",
    "    jdbc_urls = {\n",
    "        \"germany\": \"jdbc:mysql://mysql1.imported-db.svc.cluster.local:3306/end2end\",\n",
    "        \"czech\": \"jdbc:mysql://mysql2.imported-db.svc.cluster.local:3306/end2end\",\n",
    "        \"swiss\": \"jdbc:mysql://mariadb.imported-db.svc.cluster.local:3306/end2end\",\n",
    "    }\n",
    "\n",
    "    # Return the JDBC URL for the specified country\n",
    "    return jdbc_urls.get(country.lower(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83bcfe-13af-4c5d-a7e5-87a8719b8d6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a function to extract the data from SQL in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9ef1d-e4d4-4d42-acb6-c9abae749939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(spark, country):\n",
    "    # Define the JDBC connection properties\n",
    "    jdbc_url = get_jdbc_url_for_country(country)\n",
    "\n",
    "    if jdbc_url is None:\n",
    "        raise ValueError(f\"JDBC URL not defined for country: {country}\")\n",
    "\n",
    "    # Specify the table name\n",
    "    table_name = f\"end2end.{country}\"\n",
    "\n",
    "    # Read data from the SQL database\n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", mysql_properties[\"user\"]) \\\n",
    "        .option(\"password\", mysql_properties[\"password\"]) \\\n",
    "        .option(\"driver\", mysql_properties[\"driver\"]) \\\n",
    "        .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c9954-ac26-41a4-abd3-18f7b04b00ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a function to convert the currency to Euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb38a7-7fe3-4c0a-a467-4b45d9ec64f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df, spark, country):\n",
    "    # Define a UDF to convert currencies to EUR\n",
    "    convert_udf = udf(lambda currency, amount: amount / CZK_TO_EUR_RATE if currency == \"CZK\" else amount / CHF_TO_EUR_RATE if currency == \"CHF\" else amount, DoubleType())\n",
    "\n",
    "    # Apply the UDFs to the DataFrame\n",
    "    corrected_df = df.withColumn(\"totalsales\", convert_udf(col(\"currency\"), col(\"totalsales\"))) \\\n",
    "                     .withColumn(\"currency\", lit(\"EUR\"))\n",
    "\n",
    "    # Show the results\n",
    "    corrected_df.show()\n",
    "\n",
    "    return corrected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a081-3198-4387-972b-d93770235000",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a function to save the data to a Delta Tables \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> Make sure the path is correct\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d436fd8-3f8b-4193-8db9-08f6804f79fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_data(df, country):\n",
    "    delta_path = f\"file:///mounts/shared-volume/shared/end2end-delta/{group_name}/{country}\"\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(delta_path):\n",
    "        os.makedirs(delta_path)\n",
    "        \n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e149b-dbac-4d5b-9e0b-dbcee401d81b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the country names & currency rates \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> Make sure the values are correct\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b9227-280f-46a9-bc3f-0a1d1bcc0e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "COUNTRY_LIST = [\"czech\", \"germany\", \"swiss\"]\n",
    "CZK_TO_EUR_RATE = 25\n",
    "CHF_TO_EUR_RATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014dbcca-2d50-4a18-b927-75a05c63ff22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the ETL for each countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698dc6a0-115f-4899-98c7-c0d31eaa8ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for country in COUNTRY_LIST:\n",
    "    # Load data from the DBs\n",
    "    df = load_data(spark, country)\n",
    "    df.show()\n",
    "\n",
    "    # Clean the data\n",
    "    cleaned_df = clean_data(df, spark, country)\n",
    "    \n",
    "    # Write the cleaned data back to the Delta Table\n",
    "    write_data(cleaned_df, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca77167-5306-4515-aa1c-1178a42f593a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check if the Delta Tables were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61cd90-dfe5-42cd-ad23-9fcc25739f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for country in COUNTRY_LIST:\n",
    "    # List files in a directory\n",
    "    files = os.listdir(f\"/mounts/shared-volume/shared/end2end-delta/{group_name}/{country}\")\n",
    "    print(\"Table:\", country)\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            full_path = os.path.join(f\"/mounts/shared-volume/shared/end2end-delta/{group_name}/{country}\", file)\n",
    "            print(\"Saved in:\", full_path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64969c-d434-47ea-813e-1f9d8df35610",
   "metadata": {},
   "source": [
    "## Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc2daf-bd72-44ca-99a1-f1b213e23f01",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Create Initial Delta Table by loading the `czech` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3bb3a-7c4f-4053-98b1-02452c20ff8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "# Set the parameters\n",
    "country = \"czech\"\n",
    "delta_path = f\"file:///mounts/shared-volume/shared/end2end-delta/{group_name}/{country}\"\n",
    "\n",
    "# Read the Delta table using the load method\n",
    "read_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Show the contents of the DataFrame\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724dfdb-8f0c-4393-ae04-cde3901a003a",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Overwrite the Delta Table with Selected Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314d2b2-0494-4cb0-8537-c1a8027eddd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only a subset of columns for the initial Delta Table\n",
    "selected_columns = [\"type\", \"unitprice\", \"qty\", \"totalsales\"]\n",
    "select_delta_path = f\"file:///mounts/shared-volume/shared/end2end-delta/{group_name}/{country}\"\n",
    "\n",
    "# Create the initial Delta Table with selected columns\n",
    "df_select = df.select(\"type\", \"unitprice\", \"unit\", \"qty\")\n",
    "df_select.write.format(\"delta\").mode(\"overwrite\").save(select_delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e35ea-4acd-44f3-bb22-41fbe805d10a",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Read the Delta Table with Selected Columns: the old columns are marked as `NULL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33efef9-815a-4203-9eed-b5d20b5404ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the Delta table using the load method\n",
    "read_df_select = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Show the contents of the DataFrame\n",
    "read_df_select.show()\n",
    "\n",
    "# Display the schema of the version 0 DataFrame\n",
    "read_df_select.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf4cc2-a772-4a34-a045-c3077fd60a63",
   "metadata": {
    "tags": []
   },
   "source": [
    "4. Display the versions available for this table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da2e54-d1bb-4e6b-bb87-dcdb22a5707b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Get the history of the Delta table\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# List all versions with timestamp\n",
    "versions_with_timestamp = history_df.select(\"version\", \"timestamp\").distinct().collect()\n",
    "\n",
    "# Display the list of versions with timestamp\n",
    "print(\"List of Delta Table Versions with Timestamp:\")\n",
    "for version_info in versions_with_timestamp:\n",
    "    version = version_info[\"version\"]\n",
    "    timestamp = version_info[\"timestamp\"]\n",
    "    print(f\"Version: {version}, Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca95c6d-e7fe-4c0a-a9ef-a842eb9750a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "5. Compare table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5d8be-de4f-424a-b0b8-31357723db06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read a specific version (e.g., version 0) of the Delta table\n",
    "read_df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_path)\n",
    "read_df_version_0.show()\n",
    "\n",
    "# Read a specific version (e.g., version 1) of the Delta table\n",
    "read_df_version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", \"2\").load(delta_path)\n",
    "read_df_version_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47730dea-6c5d-42de-90ad-e180dfb81672",
   "metadata": {
    "tags": []
   },
   "source": [
    "6. Rollback to Initial Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b42536-d830-4940-b2ef-7914fb670b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read a specific version (e.g., version 0) of the Delta table\n",
    "read_df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_path)\n",
    "\n",
    "# If you want to perform further actions or overwrite the current Delta table:\n",
    "# Overwrite the current Delta table with version 0 data\n",
    "read_df_version_0.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27781d28-8213-4bf1-8d9e-cb210fcab1b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "7. Read the Delta Table with Selected Columns: the old columns are back in the current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6a93e-5494-4e44-a103-3e4c8ca9c19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the Delta table using the load method\n",
    "read_df_select = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Show the contents of the DataFrame\n",
    "read_df_select.show()\n",
    "\n",
    "# Display the schema of the version 0 DataFrame\n",
    "read_df_select.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ceda3-dba8-4695-9dfb-8309730e5e94",
   "metadata": {
    "tags": []
   },
   "source": [
    "8. Display the versions available for this table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbcc34-5d49-4d0c-94ff-ebc929ae2133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Get the history of the Delta table\n",
    "history_df = delta_table.history()\n",
    "\n",
    "# List all versions with timestamp\n",
    "versions_with_timestamp = history_df.select(\"version\", \"timestamp\").distinct().collect()\n",
    "\n",
    "# Display the list of versions with timestamp\n",
    "print(\"List of Delta Table Versions with Timestamp:\")\n",
    "for version_info in versions_with_timestamp:\n",
    "    version = version_info[\"version\"]\n",
    "    timestamp = version_info[\"timestamp\"]\n",
    "    print(f\"Version: {version}, Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318187ca-e5ee-4846-b0a9-293efe1f59d9",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece998e3-19d0-4360-8eb4-3c7dbf04b771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
