{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68919b67-6a24-43b4-af82-7ab48d31457e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What we will cover:\n",
    "- Preprocess the data\n",
    "- Load the data\n",
    "- Train the model and store the artifacts & metrics to Mlflow\n",
    "- Prediction test to validate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490016b4-5cb1-4699-9c00-ecfc8a19cd23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prerequisites \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> Make sure it's valid\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145ffd3d-409d-4e3b-9935-23fb795b3f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01.exploring_data_with_spark.ipynb   exercise5.ipynb\n",
      " 02.query_with_ezpreso.ipynb          exercise6.ipynb\n",
      " 03.visualizing_data_superset.ipynb   exercise7.ipynb\n",
      " create_csv.py                        final_challenge.ipynb\n",
      " currency_conversion.json             \u001b[0m\u001b[01;34mimages\u001b[0m/\n",
      " \u001b[01;34mdata\u001b[0m/                                \u001b[01;34mold\u001b[0m/\n",
      "\u001b[01;34m'end2end application'\u001b[0m/                requirements.txt\n",
      " exercise4.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls /mnt/shared/end2end-main-exercises/exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129f625-a3e9-4b4b-a95e-ee05fbe6ba0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parametes\n",
    "\n",
    "# adapt to your EZUA Domain name\n",
    "EZAF_ENV = \"i007ua.tryezmeral.com\"\n",
    "# path to end2end demo (not data)\n",
    "end2end_path = '/mnt/datasources/datafabric/ezua/end2end/' \n",
    "# path to data for model training, etc.\n",
    "path = '/mnt/datasources/datafabric/ezua/end2end-data/fruits/' \n",
    "# path to GROUP INDIVIDUAL data for model training, etc.\n",
    "group_data_path = '/mnt/datasources/datafabric/ezua/end2end-group-data/' \n",
    "# experiment name prefix for mlflow\n",
    "experiment_name = \"end2end-retail-demo\"\n",
    "model_name = \"end2end-retail-demo\"\n",
    "g_model_name = group_name + \"-\" + model_name\n",
    "# artifact_path = \"end2end-retail-demo\"\n",
    "artifact_path = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa60a94-ad02-4e1c-87ac-0b19bbfe345f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validating the setup \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important</b> DO NOT CHANGE\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe1a06-28c2-4f32-975c-4ee33af5eaff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## check if directory group data directories exists and create if not\n",
    "\n",
    "if not os.path.exists(group_data_path + group_name):\n",
    "    os.makedirs(group_data_path + group_name) \n",
    "\n",
    "if not os.path.exists(group_data_path + group_name + \"/train\"):\n",
    "    os.makedirs(group_data_path + group_name + \"/train\") \n",
    "\n",
    "if not os.path.exists(group_data_path + group_name + \"/test\"):\n",
    "    os.makedirs(group_data_path + group_name + \"/test\") \n",
    "\n",
    "if not os.path.exists(group_data_path + group_name + \"/validation\"):\n",
    "    os.makedirs(group_data_path + group_name + \"/validation\") \n",
    "\n",
    "## check if directories for model training exists\n",
    "\n",
    "dirExist_path = os.path.exists(path)\n",
    "dirExist_end2end_path = os.path.exists(end2end_path)\n",
    "dirExist_group_data_path = os.path.exists(group_data_path)\n",
    "\n",
    "if dirExist_path and dirExist_end2end_path and dirExist_group_data_path:\n",
    "    print()\n",
    "else:\n",
    "    print(\"Do not run this notebook further, dataset is missing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9589dae-75c1-49fd-8ea8-72dc42c2c429",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import required libraries & refresh token\n",
    "- Ignore the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f8bf7-ed2c-4c87-934c-9c79e6a63a93",
   "metadata": {
    "papermill": {
     "duration": 7.613981,
     "end_time": "2023-03-13T20:40:28.950010",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.336029",
     "status": "completed"
    },
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow  # Choose either mlflow.tensorflow or mlflow.keras based on your needs\n",
    "import os\n",
    "import urllib3\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from keras.callbacks import CSVLogger\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d5398-27f8-4f4e-8aab-a812e31012d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "`%update_token` is used to refresh the access to the platform. It might be needed to run it in case the exercise is taking too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ce1dd-6302-4237-8c50-ffb7d65c504f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13eb81c",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Preprocessing & Loading<a class=\"anchor\" id=\"1\"></a><a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4024d5-fed5-476c-80bc-bf3968b25975",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load images from the folder and use folder name as Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549269f5-ea2e-4ee3-8b0a-e933f28e60c1",
   "metadata": {
    "tags": [
     "block:load_preprocess_images"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a Path object for the training directory and get a list of all .jpg files in the directory\n",
    "train_dir = Path(path + 'train')\n",
    "train_filepaths = [p for p in train_dir.glob('**/*') if p.suffix.lower() in ['.jpg', '.jpeg', '.png'] and not p.name.startswith('.')]\n",
    "\n",
    "# Create a Path object for the testing directory and get a list of all .jpg files in the directory\n",
    "test_dir = Path(path + 'test')\n",
    "test_filepaths = [p for p in test_dir.glob('**/*') if p.suffix.lower() in ['.jpg', '.jpeg', '.png'] and not p.name.startswith('.')]\n",
    "\n",
    "# Create a Path object for the validation directory and get a list of all .jpg files in the directory\n",
    "val_dir = Path(path + 'validation')\n",
    "val_filepaths = [p for p in val_dir.glob('**/*') if p.suffix.lower() in ['.jpg', '.jpeg', '.png'] and not p.name.startswith('.')]\n",
    "\n",
    "# Define a function to create a DataFrame with filepaths and labels for a given list of filepaths\n",
    "def proc_img(filepath):\n",
    "    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n",
    "    \"\"\"\n",
    "    # Remove subfolders created by jupyter\n",
    "    filepath = [x for x in filepath if not \".ipynb_checkpoints\" in str(x)]\n",
    "\n",
    "    # Get the labels from the filepath by splitting on the directory separator and taking the second-to-last element\n",
    "    labels = [str(filepath[i]).split(\"/\")[-2] \\\n",
    "              for i in range(len(filepath)) \\\n",
    "              if not str(filepath[i]).split(\"/\")[-2].startswith('.')]\n",
    "\n",
    "\n",
    "    # Convert the list of filepaths to a pandas Series object\n",
    "    filepath = pd.Series(filepath, name='Filepath',dtype=str)\n",
    "\n",
    "    # Convert the list of labels to a pandas Series object\n",
    "    labels = pd.Series(labels, name='Label',dtype=str)\n",
    "\n",
    "    # Concatenate the filepaths and labels into a DataFrame\n",
    "    df = pd.concat([filepath, labels], axis=1)\n",
    "        \n",
    "    # Shuffle the DataFrame and reset the index\n",
    "    df = df.sample(frac=1).reset_index(drop = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Call the proc_img function on the training filepaths to create a DataFrame for training\n",
    "train_df = proc_img(train_filepaths)\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Call the proc_img function on the testing filepaths to create a DataFrame for testing\n",
    "test_df = proc_img(test_filepaths)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# Call the proc_img function on the validation filepaths to create a DataFrame for validation\n",
    "val_df = proc_img(val_filepaths)\n",
    "val_df.dropna(inplace=True)\n",
    "\n",
    "if {len(train_df.Label.unique())} != {len(test_df.Label.unique())} != {len(val_df.Label.unique())}:\n",
    "    print('incorrect amount of Labels, please do not continue...')\n",
    "\n",
    "\n",
    "print()\n",
    "print('#### Training set ####')\n",
    "print()\n",
    "print('-- Training set --\\n')\n",
    "print(f'Number of pictures: {train_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(train_df.Label.unique())}\\n')\n",
    "print(f'Labels: {train_df.Label.unique()}')\n",
    "print()\n",
    "print('#### Test set ####')\n",
    "print()\n",
    "print('-- Test set --\\n')\n",
    "print(f'Number of pictures: {test_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(test_df.Label.unique())}\\n')\n",
    "print(f'Labels: {test_df.Label.unique()}')\n",
    "print()\n",
    "print('#### Validation set ####')\n",
    "print()\n",
    "print('-- Validate set --\\n')\n",
    "print(f'Number of pictures: {val_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(val_df.Label.unique())}\\n')\n",
    "print(f'Labels: {val_df.Label.unique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493e33c",
   "metadata": {
    "papermill": {
     "duration": 0.031338,
     "end_time": "2023-03-13T20:40:29.032984",
     "exception": false,
     "start_time": "2023-03-13T20:40:29.001646",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# The DataFrame with the filepaths in one column and the labels in the other one\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dad0a-5661-4bef-beea-92bb9853b6cc",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Drop duplicates and diplay 1 item of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22f07f",
   "metadata": {
    "papermill": {
     "duration": 12.274186,
     "end_time": "2023-03-13T20:40:41.317059",
     "exception": false,
     "start_time": "2023-03-13T20:40:29.042873",
     "status": "completed"
    },
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with one Label of each category\n",
    "df_unique = train_df.copy().drop_duplicates(subset=[\"Label\"]).reset_index()\n",
    "\n",
    "# Display some pictures of the dataset\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15, 10),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(df_unique):\n",
    "        ax.imshow(plt.imread(df_unique.iloc[i]['Filepath']))\n",
    "        ax.set_title(df_unique.iloc[i]['Label'], fontsize = 12)\n",
    "plt.tight_layout(pad=0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22622444",
   "metadata": {
    "papermill": {
     "duration": 0.028726,
     "end_time": "2023-03-13T20:40:41.374706",
     "exception": false,
     "start_time": "2023-03-13T20:40:41.345980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69182377-1c96-48e9-b170-36f4ca74e2a5",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setting up data generators for training, validation, and testing image datasets using the MobileNetV2 preprocessing function\n",
    "- Utilizes tf.keras.preprocessing.image.ImageDataGenerator for real-time data augmentation during training.\n",
    "- The training images are loaded from a Pandas DataFrame using the specified 'Filepath' column for input data and 'Label' column for output data.\n",
    "- Images are resized to (224, 224) pixels, and the MobileNetV2 preprocessing function is applied.\n",
    "- Data augmentation techniques like rotation, zoom, shift, shear, and horizontal flip are employed to enhance the diversity of the training dataset.\n",
    "- Batches of 32 images are generated, and the order is shuffled for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c69e91-0867-4891-affc-ede2dd1fd886",
   "metadata": {
    "papermill": {
     "duration": 1.1483,
     "end_time": "2023-03-13T20:40:42.551388",
     "exception": false,
     "start_time": "2023-03-13T20:40:41.403088",
     "status": "completed"
    },
    "tags": [
     "block:loadimages",
     "prev:load_preprocess_images"
    ]
   },
   "outputs": [],
   "source": [
    "# Create an image data generator for preprocessing train images using MobileNetV2\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "# Generate a flow of images and labels from a Pandas dataframe for training\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df, # Use the specified Pandas dataframe\n",
    "    x_col='Filepath', # Use the 'Filepath' column as the input (x) data\n",
    "    y_col='Label', # Use the 'Label' column as the output (y) data\n",
    "    target_size=(224, 224), # Resize the images to the specified dimensions\n",
    "    color_mode='rgb', # Use RGB color mode\n",
    "    class_mode='categorical', # Use categorical classification\n",
    "    batch_size=32, # Generate batches of 32 images at a time\n",
    "    shuffle=True, # Shuffle the order of the images\n",
    "    seed=0, # Use a fixed seed for reproducibility\n",
    "    rotation_range=30, # Randomly rotate images up to 30 degrees\n",
    "    zoom_range=0.15, # Randomly zoom images up to 15%\n",
    "    width_shift_range=0.2, # Randomly shift images horizontally up to 20%\n",
    "    height_shift_range=0.2, # Randomly shift images vertically up to 20%\n",
    "    shear_range=0.15, # Randomly apply shearing transformations to images\n",
    "    horizontal_flip=True, # Randomly flip images horizontally\n",
    "    fill_mode=\"nearest\", # Use the nearest pixel to fill any empty spaces created by image transformations\n",
    ")\n",
    "# Create an image data generator for preprocessing validation images using MobileNetV2\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "# Generate a flow of images and labels from a Pandas dataframe for validation\n",
    "val_images = val_generator.flow_from_dataframe(\n",
    "    dataframe=val_df, # Use the specified Pandas dataframe\n",
    "    x_col='Filepath', # Use the 'Filepath' column as the input (x) data\n",
    "    y_col='Label', # Use the 'Label' column as the output (y) data\n",
    "    target_size=(224, 224), # Resize the images to the specified dimensions\n",
    "    color_mode='rgb', # Use RGB color mode\n",
    "    class_mode='categorical', # Use categorical classification\n",
    "    batch_size=32, # Generate batches of 32 images at a time\n",
    "    shuffle=True, # Shuffle the order of the images\n",
    "    seed=0, # Use a fixed seed for reproducibility\n",
    "    rotation_range=30, # Randomly rotate images up to 30 degrees\n",
    "    zoom_range=0.15, # Randomly zoom images up to 15%\n",
    "    width_shift_range=0.2, # Randomly shift images horizontally up to 20%\n",
    "    height_shift_range=0.2, # Randomly shift images vertically up to 20%\n",
    "    shear_range=0.15, # Randomly apply shearing transformations to images\n",
    "    horizontal_flip=True, # Randomly flip images horizontally\n",
    "    fill_mode=\"nearest\" # Use the nearest pixel to fill any empty spaces created by image transformations\n",
    ")\n",
    "# Create an image data generator for preprocessing test images using MobileNetV2\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "# Generate a flow of images and labels from a Pandas dataframe\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df, # Use the specified Pandas dataframe\n",
    "    x_col='Filepath', # Use the 'Filepath' column as the input (x) data\n",
    "    y_col='Label', # Use the 'Label' column as the output (y) data\n",
    "    target_size=(224, 224), # Resize the images to the specified dimensions\n",
    "    color_mode='rgb', # Use RGB color mode\n",
    "    class_mode='categorical', # Use categorical classification\n",
    "    batch_size=32, # Generate batches of 32 images at a time\n",
    "    shuffle=False # Do not shuffle the order of the images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017bf07-0949-4a3b-8d56-76f6b372ec2a",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load the pretained model\n",
    "- Load MobileNetV2 model pretrained on ImageNet with input size (224, 224, 3).\n",
    "- Configure the model for feature extraction (excluding top layer) and freeze its weights for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdb4f7",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 4.031283,
     "end_time": "2023-03-13T20:40:46.611646",
     "exception": false,
     "start_time": "2023-03-13T20:40:42.580363",
     "status": "completed"
    },
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the pretained model\n",
    "pretrained_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")\n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc7ff4",
   "metadata": {
    "papermill": {
     "duration": 0.028545,
     "end_time": "2023-03-13T20:40:46.668800",
     "exception": false,
     "start_time": "2023-03-13T20:40:46.640255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Train the model<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1563319-276e-4afd-bee9-cd522d11a2ff",
   "metadata": {
    "papermill": {
     "duration": 0.005834,
     "end_time": "2023-03-13T20:40:21.330048",
     "exception": false,
     "start_time": "2023-03-13T20:40:21.324214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.0 Set parameters for model training - feel free to adjust for the best results in terms of training duration but also accuracy of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5546f-df6c-4268-afe1-0c260f27848f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs to train the model\n",
    "param_epoch = 15\n",
    "\n",
    "# Define the batch size to use for training and validation\n",
    "param_batch_size = 32\n",
    "\n",
    "# Define the number of epochs to wait before early stopping if the validation loss does not improve\n",
    "param_patience = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b96d1b-f324-4ba4-a7c7-745cea06d98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 MLflow config setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e07b9-be28-41fc-93f7-cafe2e439fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%update_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906637b6-661a-4355-8ee0-28bfbf371282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Set up an experiment with set_exp from ezmllib.mlflow\n",
    "exp_name = group_name + \"-\" + experiment_name\n",
    "\n",
    "# Search for experiments that match the group name \n",
    "experiments = mlflow.search_experiments(view_type=3, order_by=[\"experiment_id\"], filter_string=\"name = '\" + exp_name + \"'\")\n",
    "\n",
    "# Check if the list is not empty before accessing its elements\n",
    "if experiments and experiments[0].lifecycle_stage == 'deleted':\n",
    "    MlflowClient().restore_experiment(experiments[0].experiment_id)\n",
    "\n",
    "# Set/create mlflow experiment and generate mlflow run name\n",
    "mlflow.set_experiment(exp_name)\n",
    "\n",
    "# Define mlflow run name\n",
    "run_name = \"end2end-retail-demo-\" + time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fae775-f2ad-42d2-8816-89fa72f84a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Model training (using Keras and Tensorflow) .... the execution of this cell takes a while ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d079e-036e-4229-a171-b645524ae198",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Extends the pretrained MobileNetV2 model with additional dense layers for transfer learning.\n",
    "- Creates a new Keras model with specified architecture and compiles it using Adam optimizer and categorical crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacde793-1e71-4a6c-8ea4-9d76e6faaabc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Get the input layer of the pretrained model\n",
    "inputs = pretrained_model.input\n",
    "\n",
    "# Add a new dense layer with 128 units and ReLU activation to the output of the pretrained model\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "\n",
    "# Add another new dense layer with 128 units and ReLU activation to the previous layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Get the number of unique labels in the training set\n",
    "labels = len(train_df.Label.unique())\n",
    "\n",
    "# Add a new dense layer with softmax activation to the previous layer to get the output of the new model\n",
    "outputs = tf.keras.layers.Dense(labels, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490c41c-e1e0-4073-82cb-711e7f282bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = (train_images.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405ba67-fbb9-4d9d-8b23-de0f9073a987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d31d9-dc1b-4c52-a280-eab59fa243e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Initiates an MLflow run, logs model architecture, and records hyperparameters.\n",
    "- Trains the model using the specified hyperparameters and early stopping callback.\n",
    "- Saves the trained model in TensorFlow Serving format.\n",
    "- Logs the saved model as an MLflow artifact and logs metrics for each epoch.\n",
    "- Metrics include training and validation accuracy and loss.\n",
    "- The model is saved with a specific version (1) for TensorFlow Serving.\n",
    "- The validation dataset is used to evaluate the model's performance on unseen data and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ade09b-cdb8-4a80-815a-39da46c8a9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start a new MLflow run\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    \n",
    "# Start a new MLflow run\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    \n",
    "    # Log the model architecture as a Keras summary\n",
    "    mlflow.autolog()\n",
    "    \n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_param(\"batch_size\", param_batch_size)\n",
    "    mlflow.log_param(\"epochs\", param_epoch)\n",
    "    mlflow.log_param(\"patience\", param_patience)\n",
    "    \n",
    "    # Get the full model path\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    artifact_uri = mlflow.get_artifact_uri(run_id)\n",
    "    #artifact_path = \"model\"\n",
    "    test_uri = \"s3://mlflow/3/{run_id}/artifacts/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n",
    "\n",
    "    # Train the model with the specified hyperparameters and callbacks\n",
    "    history = model.fit(\n",
    "        train_images,\n",
    "        validation_data=val_images,\n",
    "        batch_size=param_batch_size,\n",
    "        epochs=param_epoch,\n",
    "        verbose='auto',\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=param_patience,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save the model in TensorFlow Serving format\n",
    "    tf.saved_model.save(model, \"tf_serving_model/1\")\n",
    "\n",
    "    # Log the saved model as an artifact\n",
    "    mlflow.log_artifact(\"tf_serving_model\")\n",
    "    \n",
    "    # Log values/metric from all epochs in mlflow\n",
    "    for item in history.history.items():\n",
    "        for value in item[1]:\n",
    "            mlflow.log_metric(item[0], value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5c3f3-af34-4f35-a4c0-b0758b741acc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Validate Accuracy by predicting on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59124e",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 106.90135,
     "end_time": "2023-03-13T21:24:09.563300",
     "exception": false,
     "start_time": "2023-03-13T21:22:22.661950",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Predict the label of the test_images\n",
    "pred = model.predict(test_images)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_images.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "y_test = [labels[k] for k in test_images.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbea73",
   "metadata": {
    "papermill": {
     "duration": 0.092389,
     "end_time": "2023-03-13T21:24:09.735823",
     "exception": false,
     "start_time": "2023-03-13T21:24:09.643434",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "labels.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596016a7",
   "metadata": {
    "papermill": {
     "duration": 0.694135,
     "end_time": "2023-03-13T21:24:10.511655",
     "exception": false,
     "start_time": "2023-03-13T21:24:09.817520",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f'Accuracy on the test set: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c25ec-a82f-4a5b-b5e8-070d0c5f4eb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Try to predict yourself using a picture from the web\n",
    "- `Fill in` the missing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada40f3d-4860-4175-9b8e-a6271a0f6cfa",
   "metadata": {
    "papermill": {
     "duration": 0.168369,
     "end_time": "2023-03-13T21:24:26.545710",
     "exception": false,
     "start_time": "2023-03-13T21:24:26.377341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(url):\n",
    "    # Preprocess the image\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize((224, 224))  \n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction, axis=-1)\n",
    "    result = labels[predicted_class[0]]\n",
    "    \n",
    "    # Display the image\n",
    "    display(img)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac911ac6-c330-42b1-8b72-381f27df8e3d",
   "metadata": {
    "papermill": {
     "duration": 0.168369,
     "end_time": "2023-03-13T21:24:26.545710",
     "exception": false,
     "start_time": "2023-03-13T21:24:26.377341",
     "status": "completed"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Example usage with an image URL\n",
    "image_url = \"https://www.telegraph.co.uk/multimedia/archive/01834/orange_1834038b.jpg?imwidth=1280\"\n",
    "\n",
    "predicted_label = predict(image_url)\n",
    "print(\"The model predicts: \" + str(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65778b-3b0d-4f5f-9626-f9e9cd05c945",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc2f0c-9e04-44e1-9bb9-30874e034883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4ab6d-b536-4539-943b-00ef3d9ec235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/mapr-252711/kubeflow/notebooks/jupyter-tensorflow-cuda-full:ezaf-fy23-q2",
   "experiment": {
    "id": "new",
    "name": "jk-fruit-demo"
   },
   "experiment_name": "jk-fruit-demo",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "10"
      },
      {
       "name": "acq_optimizer",
       "value": "auto"
      },
      {
       "name": "acq_func",
       "value": "gp_hedge"
      },
      {
       "name": "base_estimator",
       "value": "GP"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "goal": 1,
     "objectiveMetricName": "stage",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "50",
       "min": "1",
       "step": "1"
      },
      "name": "param_epoch",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "list": [
        "32",
        "64"
       ]
      },
      "name": "param_batch_size",
      "parameterType": "categorical"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "1",
       "step": "1"
      },
      "name": "param_patience",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": false,
   "pipeline_description": "fruit-veg",
   "pipeline_name": "fruit-veg",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "storage_class_name": "dataplatform",
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/mnt/shared/",
     "name": "kubeflow-pipeline",
     "size": 1,
     "size_type": "Gi",
     "snapshot": false,
     "snapshot_name": "",
     "type": "pvc"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2679.282234,
   "end_time": "2023-03-13T21:24:51.692744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-13T20:40:12.410510",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
