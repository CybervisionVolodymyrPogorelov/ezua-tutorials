{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3324b7d0-515d-46a7-ac00-03e788d5e96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vector Stores: Embedding and Storing Documents in a Latent Space\n",
    "\n",
    "In this Jupyter Notebook, you explore a foundational element of a question-answering system: the Vector Store. The\n",
    "Vector Store serves as the key component that allows you to efficiently retrieve relevant context from a corpus of\n",
    "documents based on a user's query, providing the backbone of the information retrieval system.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/documents.jpg\" alt=\"documents\" style=\"width:100%\">\n",
    "  <figcaption>\n",
    "      Photo by <a href=\"https://unsplash.com/@anniespratt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Annie Spratt</a> on <a href=\"https://unsplash.com/photos/5cFwQ-WMcJU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The approach you will use involves transforming each document into a high-dimensional numerical\n",
    "representation known as an \"embedding\", using a fine-tuned [BGE-M3](https://arxiv.org/abs/2402.03216) embeddings model.\n",
    "This process is sometimes referred to as \"embedding\" the document in a latent space. The latent space here is a\n",
    "high-dimensional space where similar documents are close to each other. The position of a document in this space is\n",
    "determined by the content and the semantic meaning it carries.\n",
    "\n",
    "Once you have these embeddings, you store them in a Vector Store. A Vector Store is an advanced AI-native database\n",
    "designed to hold these high-dimensional vectors, index them, and provide efficient search capabilities. This enables you\n",
    "to quickly identify documents in your corpus that are semantically similar to a given query, which will also be\n",
    "represented as a vector in the same latent space. For this example, you will use [Chroma](https://www.trychroma.com/),\n",
    "a popular open source vector database.\n",
    "\n",
    "The following cells in this Notebook guides you through the process of creating such a Vector Store. You start by\n",
    "generating embeddings for each document, then you move on to storing these embeddings in a Vector Store, and finally,\n",
    "you see how easy it is to to retrieve documents from the Vector Store based on a query.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Download the Embeddings Model](#download-the-embeddings-model)\n",
    "1. [Load the Documents](#load-the-documents)\n",
    "1. [Document Processing](#document-processing-chunking-text-for-the-language-model)\n",
    "1. [Generate and Store Embeddings](#generating-embeddings--storing-them-in-chroma)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df209f-e471-45d8-ad58-ad5c34819ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "from embeddings import EmbeddingsModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787d603-eea5-446b-bd7e-2733b0137618",
   "metadata": {},
   "source": [
    "# Download the Embeddings Model\n",
    "\n",
    "The initial step in this process involves downloading the fine-tuned embeddings model. This is a vital step, as you will require the model to both create the Vector Store and deploy it for inference with KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b348281-6967-4fb2-aa27-5d779962f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are behind a proxy, do not forget to set your `https_proxy` and `HTTPS_PROXY` environment variable.\n",
    "# os.environ[\"https_proxy\"] = \"\"\n",
    "# os.environ[\"HTTPS_PROXY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097c431-da91-4811-8a48-200131032ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bge-m3  # create a directorry to download the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94c8d0-5ae8-45a7-b1c6-5c6108c2b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ezmeral-artifacts.s3.us-east-2.amazonaws.com/bge-m3.tar.gz  # download the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80041c5c-5e4f-474a-a1a8-798b057d5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv bge-m3.tar.gz bge-m3  # move the embeddings model tarball into the right directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42118141-8339-468b-864f-c2e1b9e5583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzf bge-m3/bge-m3.tar.gz -C bge-m3 # extract the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65d7d0-b5bc-4b02-b5b7-4e8575b77940",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm bge-m3/bge-m3.tar.gz  # remove the tarball you downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea076619-2576-4154-ad50-0775b84a4359",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the Documents\n",
    "\n",
    "The next cells contain a set of helper functions designed to load JSON documents from a specified directory. These\n",
    "functions are essential for preparing your data before embedding it into the high-dimensional latent space. By\n",
    "running the following cells, you have a list of documents ready to be processed and embedded in the latent space.\n",
    "This forms your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41fcf7-9e8c-4198-9552-da1d23dd3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a5db7-b689-437e-b756-60ed7afc2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ezua_loader = JSONLoader(\n",
    "    file_path='./documents/EzUA.json',\n",
    "    jq_schema='.[].content',\n",
    "    text_content=False)\n",
    "\n",
    "ezua_data = ezua_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f951948-8459-48e1-8187-1ba5b8a4141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ezdf_loader = JSONLoader(\n",
    "    file_path='./documents/EzDF.json',\n",
    "    jq_schema='.[].content',\n",
    "    text_content=False)\n",
    "\n",
    "ezdf_data = ezdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97972d83-3d4c-40c4-857b-a2bd8eb4d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlde_loader = JSONLoader(\n",
    "    file_path='./documents/MLDE.json',\n",
    "    jq_schema='.[].content',\n",
    "    text_content=False)\n",
    "\n",
    "mlde_data = mlde_loader.load()\n",
    "\n",
    "mlde_data_filtered = list(filter(lambda doc: doc.page_content != \"\", mlde_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da034b04-2882-43cd-8fe9-00279747a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mldm_loader = JSONLoader(\n",
    "    file_path='./documents/MLDM.json',\n",
    "    jq_schema='.[].body',\n",
    "    text_content=False)\n",
    "\n",
    "mldm_data = mldm_loader.load()\n",
    "\n",
    "mldm_data_filtered = list(filter(lambda doc: doc.page_content != \"\", mldm_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8683547-5a20-4fda-8e54-47ce46a776fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ezua_data + ezdf_data + mlde_data_filtered + mldm_data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b8-52d1-468d-a28c-7384b90f553f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document Processing: Chunking Text for the Language Model\n",
    "\n",
    "In this section of the Notebook, you process the documents by splitting them into chunks. This operation is crucial when\n",
    "working with Large Language Models (LLMs), as these models have a maximum limit on the number of tokens (words or pieces\n",
    "of words) they can process at once. This limit is often referred to as the model's \"context window\".\n",
    "\n",
    "In this example, you split each document into segments that are at most `500` tokens long. You use the LangChain's\n",
    "`RecursiveCharacterTextSplitter`, which, by default, splits each document when it encounters two consecutive newline\n",
    "characters, represented as `\\n\\n`. Furthermore, each segment is distinct, meaning there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389f2b4-9bde-48b7-a87b-eee356c0fae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_docs(docs: list, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    \"\"\"Load the documents and split them into chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    return texts\n",
    "\n",
    "texts = process_docs(docs, chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e838-1746-4100-bd22-30e03b36c3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating Embeddings & Storing them in Chroma\n",
    "\n",
    "In this section of the Notebook, you use the embeddings model to transform your documents into semantically\n",
    "meaningful vectors.\n",
    "\n",
    "By leveraging this model and the Chroma database interface provided by LangChain, you can embed your documents into\n",
    "a latent space and subsequently store the results in a Vector Store. At this step, the model processes batches of documents\n",
    "in order, thus it may take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51f4b2-ff73-4bb7-a04c-bd1612db0208",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = EmbeddingsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c988d-dc14-48da-ba95-7232142f81b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = Chroma(embedding_function=embeddings, persist_directory=f\"{os.getcwd()}/db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5478dcc-9ba9-444e-9c0d-1373ece594b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # Documents to process simultaneously\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    db.add_documents(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02faf0fb-f000-4696-a651-b621527621e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f5bcc-c6ee-416c-8893-b30a69f2f054",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, you can test the accuracy of the document retrieval mechanism by providing a simple query. Chroma will return\n",
    "with the four most similar documents by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5699cd9-93b3-4513-8cb6-2d492ff5a06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How can I get started with HPE Ezmeral Unified Anaytics?\"\n",
    "matches = db.similarity_search(query); matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb1b93-567e-4c2e-b3f9-48a3b1d43fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully embedded your documents into a high-dimensional latent space\n",
    "and stored these embeddings in a Vector Store. By accomplishing this, you've transformed unstructured text data into a\n",
    "structured form that can power a robust question-answering system.\n",
    "\n",
    "However, your journey doesn't end here. Now that you have the Vector Store ready, the next step is to create an\n",
    "Inference Service (ISVC) that can leverage this store to provide context to user queries. For this, you use KServe, a\n",
    "flexible, cloud-native platform for serving Machine Learning models.\n",
    "\n",
    "In the next Notebook, you will configure two ISVCs: a custom ISVC using KServe to deploy the Chroma Database, and\n",
    "another one backed by with the Triton Inference Service to facilitate the embeddings model connection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
