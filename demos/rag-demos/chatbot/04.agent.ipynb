{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Agent\n",
    "\n",
    "Welcome to the fourth part of the tutorial series on building a chatbot over a corpus of private\n",
    "documents using Large Language Models (LLMs).\n",
    "\n",
    "In this Notebook, you set up an intelligent agent that knows how to triage a user's query to the\n",
    "right model and return the answer. If the user's query is about a document, the agent will use the\n",
    "document retrieval model to find the most relevant document. If the user's query is about a question\n",
    "that should be answered by looking into an SQL database, the agent will use the SQL model to create\n",
    "the proper SQL statement and return the answer.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Creating the Agent Deployment](#creating-the-agent-deployment)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b51ccb-f1d3-4132-a88b-62b84d90b11a",
   "metadata": {},
   "source": [
    "# Creating the Agent Deployment\n",
    "\n",
    "Apply the deployment spec below to create the agent deployment. You will need to provide the\n",
    "following environment variables:\n",
    "\n",
    "- `SQL_URL`: The URL of the SQLCoder model.\n",
    "- `LLM_URL`: The URL of the Large Language Model.\n",
    "- `EMBEDDINGS_URL`: The URL of the Document Embeddings model.\n",
    "\n",
    "You also need to provide the image of the agent. You can build this image using the dockerfile in\n",
    "the `dockerfiles/agent` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"...\"  # image name\n",
    "sql_url = \"...\"  # sql url endpoint\n",
    "llm_url = \"...\"  # llm url endpoint\n",
    "embeddings_url = \"...\"  # embeddings url endpoint\n",
    "\n",
    "agent_deploy = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  labels:\n",
    "    io.kompose.service: router\n",
    "  name: router\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      io.kompose.service: router\n",
    "  strategy:\n",
    "    rollingUpdate:\n",
    "      maxSurge: 25%\n",
    "      maxUnavailable: 25%\n",
    "    type: RollingUpdate\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        io.kompose.network/nvidia-llm: \"true\"\n",
    "        io.kompose.service: router\n",
    "    spec:\n",
    "      containers:\n",
    "      - env:\n",
    "        - name: LANGSERVE_HOST\n",
    "          value: 0.0.0.0\n",
    "        - name: LANGSERVE_PORT\n",
    "          value: \"9000\"\n",
    "        - name: SQL_URL\n",
    "          value: {0}\n",
    "        - name: LLM_URL\n",
    "          value: {1}\n",
    "        - name: EMBEDDINGS_URL\n",
    "          value: {2}\n",
    "        image: {3}\n",
    "        imagePullPolicy: IfNotPresent\n",
    "        name: chain-server\n",
    "        ports:\n",
    "        - containerPort: 9000\n",
    "          hostPort: 9000\n",
    "          name: 9000-tcp\n",
    "          protocol: TCP\n",
    "      restartPolicy: Always\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  labels:\n",
    "    io.kompose.service: router\n",
    "  name: router\n",
    "spec:\n",
    "  ports:\n",
    "  - name: 9000-tcp\n",
    "    port: 9000\n",
    "    protocol: TCP\n",
    "    targetPort: 9000\n",
    "  selector:\n",
    "    io.kompose.service: router\n",
    "  type: ClusterIP\n",
    "\"\"\".format(sql_url, llm_url, embeddings_url, image)\n",
    "\n",
    "with open(\"agent-deploy.yaml\", \"w\") as f:\n",
    "    f.write(agent_deploy)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"agent-deploy.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully\n",
    "deployed an intelligent agent that can triage a user's query to the right model and return the\n",
    "right answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
