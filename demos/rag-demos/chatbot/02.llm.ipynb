{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f891729d-9030-4d9e-a7be-e81386ae820f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Large Language Model Inference Service\n",
    "\n",
    "Welcome to the second part of the tutorial series on building a chatbot over a corpus of private\n",
    "documents using Large Language Models (LLMs). The previous Notebooks walked you through the process\n",
    "of deploying an embeddings model and using it to populate a Vector Store.\n",
    "\n",
    "Now, you're moving towards the next crucial step: creating an Inference Service (ISVC) for the LLM.\n",
    "This ISVC is the centerpiece of the chatbot application, working in tandem with the Vector Store\n",
    "to deliver comprehensive and accurate answers to user queries.\n",
    "\n",
    "In this Notebook, you set up this LLM ISVC. You will deploy a new KServe serving runtime and deploy\n",
    "an KServe ISVC that uses an NVIDIA NIM backend.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Creating the Inference Service](#creating-the-inference-service)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22867e3-a69c-488a-819e-cced462be9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b51ccb-f1d3-4132-a88b-62b84d90b11a",
   "metadata": {},
   "source": [
    "# Creating the Inference Service\n",
    "\n",
    "As before, first you need to apply a custom KServe serving runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_runtime_image = \"...\"  # The image of the serving runtime here\n",
    "\n",
    "serving_runtime_llm = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1alpha1\n",
    "kind: ClusterServingRuntime\n",
    "metadata:\n",
    "  name: nvidia-nim-llm-24.02.day0\n",
    "spec:\n",
    "  annotations:\n",
    "    prometheus.kserve.io/path: /metrics\n",
    "    prometheus.kserve.io/port: \"8002\"\n",
    "  containers:\n",
    "  - args:\n",
    "    - |\n",
    "      ln -s /mnt/models/* /model-store/; \\\n",
    "      echo 'engine:\n",
    "        model: /model-store\n",
    "        tensor_parallel_size: {{.Annotations.num_gpus}}\n",
    "        dtype: float16' > /tmp/model_config.yaml; \\\n",
    "      nim_vllm --model_name={{.Annotations.nim_model_name}} \\\n",
    "        --openai_port=8000 \\\n",
    "        --model_config /tmp/model_config.yaml\n",
    "    command:\n",
    "    - /bin/sh\n",
    "    - -c\n",
    "    image: {0}\n",
    "    name: kserve-container\n",
    "    ports:\n",
    "    - containerPort: 8000\n",
    "      protocol: TCP\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"8\"\n",
    "        memory: 128Gi\n",
    "        nvidia.com/gpu: 1\n",
    "      requests:\n",
    "        cpu: \"4\"\n",
    "        memory: 64Gi\n",
    "        nvidia.com/gpu: 1\n",
    "    securityContext:\n",
    "      runAsUser: 4474987\n",
    "    volumeMounts:\n",
    "    - mountPath: /dev/shm\n",
    "      name: dshm\n",
    "  imagePullSecrets:\n",
    "  - name: ngc-secret\n",
    "  protocolVersions:\n",
    "  - v2\n",
    "  - grpc-v2\n",
    "  supportedModelFormats:\n",
    "  - autoSelect: true\n",
    "    name: nvidia-nim-llm\n",
    "    priority: 1\n",
    "    version: 24.02.day0\n",
    "  volumes:\n",
    "  - emptyDir:\n",
    "      medium: Memory\n",
    "      sizeLimit: 128Gi\n",
    "    name: dshm\n",
    "\"\"\".format(serving_runtime_image)\n",
    "\n",
    "with open(\"serving-runtime-llm.yaml\", \"w\") as f:\n",
    "    f.write(serving_runtime_llm)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"serving-runtime-llm.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4d007",
   "metadata": {},
   "source": [
    "Next, deploy the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_uri = \"...\"  # The storage URI here\n",
    "\n",
    "llm_isvc = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  annotations:\n",
    "    autoscaling.knative.dev/target: \"10\"\n",
    "    nim_model_name: llama2-7b-chat\n",
    "    num_gpus: \"1\"\n",
    "  name: llama2-7b-chat-1xgpu-day0\n",
    "spec:\n",
    "  predictor:\n",
    "    minReplicas: 1\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: nvidia-nim-llm\n",
    "      name: \"\"\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: \"8\"\n",
    "          memory: 64Gi\n",
    "          nvidia.com/gpu: \"1\"\n",
    "        requests:\n",
    "          cpu: \"6\"\n",
    "          memory: 48Gi\n",
    "          nvidia.com/gpu: \"1\"\n",
    "      runtime: nvidia-nim-llm-24.02.day0\n",
    "      storageUri: {0}\n",
    "\"\"\".format(storage_uri)\n",
    "\n",
    "with open(\"llm-isvc.yaml\", \"w\") as f:\n",
    "    f.write(llm_isvc)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"llm-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20235af7-7b47-4b68-8f6d-f0b01b0c23e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations on completing this crucial step in this tutorial series! You've successfully\n",
    "deployed an LLM with KServe, using a custom NVIDIA NIM serving runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
