{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3324b7d0-515d-46a7-ac00-03e788d5e96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vector Stores: Embedding and Storing Documents in a Latent Space\n",
    "\n",
    "In this Jupyter Notebook, you explore a foundational element of a question-answering system: the\n",
    "Vector Store. The Vector Store serves as the key component that allows you to efficiently retrieve\n",
    "relevant context from a corpus of documents based on a user's query, providing the backbone of the\n",
    "information retrieval system.\n",
    "\n",
    "The approach you will use involves transforming each document into a high-dimensional numerical\n",
    "representation known as an \"embedding\", using a fine-tuned embeddings model. This process is\n",
    "sometimes referred to as \"embedding\" the document in a latent space. The latent space here is a\n",
    "high-dimensional space where similar documents are close to each other. The position of a document\n",
    "in this space is determined by the content and the semantic meaning it carries.\n",
    "\n",
    "Once you have these embeddings, you store them in a Vector Store. A Vector Store is an advanced\n",
    "AI-native database designed to hold these high-dimensional vectors, index them, and provide\n",
    "efficient search capabilities. This enables you to quickly identify documents in your corpus that\n",
    "are semantically similar to a given query, which will also be represented as a vector in the same\n",
    "latent space. For this example, you will use [FAISS](https://ai.meta.com/tools/faiss/), a popular\n",
    "open source vector database.\n",
    "\n",
    "The following cells in this Notebook guide you through the process of creating such a Vector Store.\n",
    "You start by generating embeddings for each document, then you move on to storing these embeddings\n",
    "in FAISS, and finally, you see how easy it is to to retrieve documents from it based on a query.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Deploy the Embeddings Model](#deploy-the-embeddings-model)\n",
    "1. [Load the Documents](#load-the-documents)\n",
    "1. [Document Processing](#document-processing-chunking-text-for-the-language-model)\n",
    "1. [Generate and Store Embeddings](#generating-embeddings--storing-them-in-chroma)\n",
    "1. [Conclusion and Next Steps](#conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df209f-e471-45d8-ad58-ad5c34819ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import IFrame, display\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from embeddings import EmbeddingsModelClient\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc04016",
   "metadata": {},
   "source": [
    "First, let's get an authentication token that we can use to invoke the embeddings model Inference\n",
    "Service (ISVC). You will need this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd6418-5b7b-4817-b810-54a83a1edcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add heading\n",
    "heading = widgets.HTML(\"<h2>Credentials</h2>\")\n",
    "display(heading)\n",
    "\n",
    "domain_input = widgets.Text(description='Username:', placeholder=\"i001ua.tryezmeral.com\")\n",
    "username_input = widgets.Text(description='Username:')\n",
    "password_input = widgets.Password(description='Password:')\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "success_message = widgets.Output()\n",
    "\n",
    "domain = None\n",
    "username = None\n",
    "password = None\n",
    "\n",
    "def submit_button_clicked(b):\n",
    "    global domain, username, password\n",
    "    domain = domain_input.value\n",
    "    username = username_input.value\n",
    "    password = password_input.value\n",
    "    with success_message:\n",
    "        success_message.clear_output()\n",
    "        print(\"Credentials submitted successfully!\")\n",
    "    submit_button.disabled = True\n",
    "\n",
    "submit_button.on_click(submit_button_clicked)\n",
    "\n",
    "# Set margin on the submit button\n",
    "submit_button.layout.margin = '20px 0 20px 0'\n",
    "\n",
    "# Display inputs and button\n",
    "display(domain_input, username_input, password_input, submit_button, success_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee917f38-7c9c-4a05-8ec9-d6e4cc933c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_url = f\"https://keycloak.{domain}/realms/UA/protocol/openid-connect/token\"\n",
    "\n",
    "data = {\n",
    "    \"username\" : username,\n",
    "    \"password\" : password,\n",
    "    \"grant_type\" : \"password\",\n",
    "    \"client_id\" : \"ua-grant\",\n",
    "}\n",
    "\n",
    "token_responce = requests.post(token_url, data=data, allow_redirects=True, verify=False)\n",
    "\n",
    "token = token_responce.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1666b",
   "metadata": {},
   "source": [
    "# Deploy the Embeddings Model\n",
    "\n",
    "First, we need to deploy the embeddings model we will use to turn documents into multi-dimensional\n",
    "vectors. For this, we will use KServe, which leverages NVIDIA NIM as a backend.\n",
    "\n",
    "The first step is to create an image pull secret, to pull the necessary images for deploying NVIDIA\n",
    "NIM models. For this, you'll need a NVCR token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcr_token = \"...\"  # Your NGC token here\n",
    "\n",
    "ngc_secret = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: ngc-secret\n",
    "type: kubernetes.io/dockerconfigjson\n",
    "data:\n",
    "  .dockerconfigjson: {0}\n",
    "\"\"\".format(nvcr_token)\n",
    "\n",
    "with open(\"ngc-secret.yaml\", \"w\") as f:\n",
    "    f.write(ngc_secret)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"ngc-secret.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e5ff5",
   "metadata": {},
   "source": [
    "Then, create a custom KServe runtime for NVIDIA NIM models. For this you need to get the name of the\n",
    "image for the runtime, and ensure that you can pull it using the image pull secret you created in\n",
    "the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9daeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_runtime_image = \"...\"  # The image of the serving runtime here\n",
    "\n",
    "serving_runtime_embeddings = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1alpha1\n",
    "kind: ClusterServingRuntime\n",
    "metadata:\n",
    "  name: nvidia-nim-embedding-24.02\n",
    "spec:\n",
    "  annotations:\n",
    "    prometheus.kserve.io/path: /metrics\n",
    "    prometheus.kserve.io/port: \"8002\"\n",
    "  containers:\n",
    "  - args:\n",
    "    - |\n",
    "      sed -i 's/checkpoint_path: .*/checkpoint_path: \"\\/mnt\\/models\\/nv-embed-qa_v4\\/NV-Embed-QA-4.nemo\"/g' /app/model_config_templates/NV-Embed-QA_template.yaml; \\\n",
    "      /app/bin/web -c /mnt/models/nv-embed-qa_v4/NV-Embed-QA-4.nemo -g /app/model_config_templates/NV-Embed-QA_template.yaml -p \"8080\"\n",
    "    command:\n",
    "    - /bin/sh\n",
    "    - -c\n",
    "    image: {0}\n",
    "    name: kserve-container\n",
    "    ports:\n",
    "    - containerPort: 8080\n",
    "      protocol: TCP\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"8\"\n",
    "        memory: 128Gi\n",
    "        nvidia.com/gpu: 1\n",
    "      requests:\n",
    "        cpu: \"4\"\n",
    "        memory: 64Gi\n",
    "        nvidia.com/gpu: 1\n",
    "    securityContext:\n",
    "      runAsUser: 4474987\n",
    "    volumeMounts:\n",
    "    - mountPath: /dev/shm\n",
    "      name: dshm\n",
    "  imagePullSecrets:\n",
    "  - name: ngc-secret\n",
    "  protocolVersions:\n",
    "  - v2\n",
    "  - grpc-v2\n",
    "  supportedModelFormats:\n",
    "  - autoSelect: true\n",
    "    name: nvidia-nim-embedding\n",
    "    priority: 1\n",
    "    version: \"24.02\"\n",
    "  volumes:\n",
    "  - emptyDir:\n",
    "      medium: Memory\n",
    "      sizeLimit: 128Gi\n",
    "    name: dshm\n",
    "\"\"\".format(serving_runtime_image)\n",
    "\n",
    "with open(\"serving-runtime-embeddings.yaml\", \"w\") as f:\n",
    "    f.write(serving_runtime_embeddings)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"serving-runtime-embeddings.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706191b",
   "metadata": {},
   "source": [
    "Finally, deploy the model. Make sure that you have the model stored in a location that the server\n",
    "can access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0055c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_uri = \"...\"  # The storage URI here\n",
    "\n",
    "embeddings_isvc = \"\"\"\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  annotations:\n",
    "    autoscaling.knative.dev/target: \"10\"\n",
    "    nim_model_name: nv-embed-qa\n",
    "  name: nv-embed-qa-4\n",
    "spec:\n",
    "  predictor:\n",
    "    minReplicas: 1\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: nvidia-nim-embedding\n",
    "      name: \"\"\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: \"8\"\n",
    "          memory: 128Gi\n",
    "          nvidia.com/gpu: \"1\"\n",
    "        requests:\n",
    "          cpu: \"6\"\n",
    "          memory: 64Gi\n",
    "          nvidia.com/gpu: \"1\"\n",
    "      runtime: nvidia-nim-embedding-24.02\n",
    "      storageUri: {0}\n",
    "\"\"\".format(storage_uri)\n",
    "\n",
    "with open(\"embeddings-isvc.yaml\", \"w\") as f:\n",
    "    f.write(embeddings_isvc)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"embeddings-isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea076619-2576-4154-ad50-0775b84a4359",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the Documents\n",
    "\n",
    "The next cells contain a set of helper functions designed to load JSON documents from a specified\n",
    "directory. These functions are essential for preparing your data before embedding it into the\n",
    "high-dimensional latent space. By running the following cells, you have a list of documents ready to\n",
    "be processed and embedded in the latent space. This forms your corpus.\n",
    "\n",
    "First, let's take a look at the documents we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_example='documents/Q2xhcmEgSG9sb3NjYW4gTUdYIDMvMjIvMjIucGRm.pdf'\n",
    "\n",
    "# visualize the pdf sample\n",
    "IFrame(pdf_example, width=900, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41fcf7-9e8c-4198-9552-da1d23dd3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCS = 100  # Number of documents to load\n",
    "pdf_folder_path = \"documents/\"\n",
    "\n",
    "documents = []\n",
    "for num_docs, file in enumerate(os.listdir(pdf_folder_path)):\n",
    "    if file.endswith('.pdf') and num_docs < NUM_DOCS:\n",
    "        pdf_path = os.path.join(pdf_folder_path, file)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09183fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[50]  # examine of the document's extracted text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b8-52d1-468d-a28c-7384b90f553f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Document Processing: Chunking Text for the Language Model\n",
    "\n",
    "In this section of the Notebook, you process the documents by splitting them into chunks. This\n",
    "operation is crucial when working with Large Language Models (LLMs), as these models have a maximum\n",
    "limit on the number of tokens (words or pieces of words) they can process at once. This limit is\n",
    "often referred to as the model's \"context window\".\n",
    "\n",
    "In this example, you split each document into segments that are at most `500` tokens long. You use\n",
    "LangChain's `RecursiveCharacterTextSplitter`, which, by default, splits each document when it\n",
    "encounters two consecutive newline characters, represented as `\\n\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389f2b4-9bde-48b7-a87b-eee356c0fae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e838-1746-4100-bd22-30e03b36c3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating Embeddings & Storing them in Chroma\n",
    "\n",
    "In this section of the Notebook, you use the embeddings model to transform your documents into\n",
    "semantically meaningful vectors.\n",
    "\n",
    "By leveraging this model and the FAISS database interface provided by LangChain, you can embed your\n",
    "documents into a latent space and subsequently store the results in a Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb5aa1-d7ed-40e1-a8e9-792d1fbf0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_NAME = domain\n",
    "DEPLOYMENT_NAME = \"nv-embed-qa-4-predictor\"\n",
    "NAMESPACE = open(\n",
    "    \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\"\n",
    ").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51f4b2-ff73-4bb7-a04c-bd1612db0208",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = EmbeddingsModelClient(\n",
    "    model_name=\"NV-Embed-QA\",\n",
    "    domain_name=DOMAIN_NAME,\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    "    token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c988d-dc14-48da-ba95-7232142f81b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
    "vectorstore.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f5bcc-c6ee-416c-8893-b30a69f2f054",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, you can test the accuracy of the document retrieval mechanism by providing a simple query.\n",
    "FAISS will return with the four most similar documents by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5699cd9-93b3-4513-8cb6-2d492ff5a06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is NVIDIA cuOpt?\"\n",
    "matches = vectorstore.similarity_search(query); matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb1b93-567e-4c2e-b3f9-48a3b1d43fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully embedded your documents into a high-dimensional latent space\n",
    "and stored these embeddings in a Vector Store. By accomplishing this, you've transformed\n",
    "unstructured text data into a structured form that can power a robust question-answering system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
