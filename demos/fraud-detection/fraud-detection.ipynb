{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Fraud Detection (Kubeflow Pipelines & KServe)**\n",
    "\n",
    "## Introduction\n",
    "Fraudulent activity has permeated multiple sectors, from e-commerce and healthcare to banking and payment systems. This\n",
    "illicit industry amasses billions every year and is on an upward trajectory. The 2018 global economic crime survey by\n",
    "PwC verifies this assertion, revealing that 49 percent of the 7,200 enterprises surveyed had fallen prey to some form\n",
    "of fraudulent conduct.\n",
    "\n",
    "In this tutorial, you'll learn how to use Kubeflow Pipelines and Kserve on HPE Ezmeral Unified Analytics through the training of a Machine Learning (ML) model to detect fraudulent banking transactions. In addition to making a detection model, you will serve the model to a custom end-user application that is analogous to any scalable front-end service, like a web or mobile app. \n",
    "\n",
    "This tutorial leverages the Banksim dataset, which is a synthetically created dataset that contains a combination of various customer payments, made at different intervals and in\n",
    "varying amounts. Through this, you will create an automated system that can detect and curtail fraudulent activities with high accuracy.\n",
    "\n",
    "![fraud-detection-banking](images/artboard.png)\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Imports and Initialization](#imports-and-initialization)\n",
    "1. [Helper Functions](#helper-functions)\n",
    "1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "1. [Data Preprocessing](#Data-Preprocessing)\n",
    "1. [Oversampling with SMOTE](#Oversampling-with-SMOTE)\n",
    "1. [K-Neighbours Classifier](#K-Neighbours-Classifier)\n",
    "1. [Random Forest Classifier](#Random-Forest-Classifier)\n",
    "1. [XGBoost Classifier](#XGBoost-Classifier)\n",
    "1. [Logistic Regression Classifier](#Logistic-Regression-Classifier)\n",
    "1. [Model Deployment](#Model-Deployment)\n",
    "1. [Prediction](#Prediction)\n",
    "1. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Imports and Initialization**\n",
    "\n",
    "First, import all the necessary packages. These packages provide the tools and\n",
    "functionalities needed to effectively process the data, train your machine learning model and evaluate its performance.\n",
    "\n",
    "### What is the scikit-learn toolkit?\n",
    "\n",
    "Scikit-learn is a powerful and versatile Python library used for machine learning. It offers a wide range of tools for building machine learning models, including regression, classification, clustering, and dimensionality reduction, among others. Scikit-learn is built upon the foundations of NumPy, SciPy, and matplotlib, making it well-integrated into the broader Python ecosystem for scientific computing. Its easy-to-use interface and extensive documentation make it highly accessible for beginners, yet it's powerful enough for experienced practitioners. Scikit-learn is known for its consistency in API design and its efficiency in handling large datasets.\n",
    "\n",
    "Later, we will see the Scikit-learn library in action, leveraging a number of machine learning techniques to build various fraud prediction models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "# System Libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "# Statitical Libraries\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Machine Learning Libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (accuracy_score, auc, classification_report,\n",
    "                             confusion_matrix, roc_curve)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress warnings\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.environ['MLFLOW_TRACKING_TOKEN']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"s3\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = 'http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = os.environ[\"AWS_ENDPOINT_URL\"]\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local S3 Bucket setup\n",
    "\n",
    "To give the pipeline access to the Banksim dataset (which has been provided and can be found in the **dataset/feed.csv** folder), load the dataset into an S3 bucket within the object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:initialize"
    ]
   },
   "outputs": [],
   "source": [
    "NAMESPACE_PATH = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "NAMESPACE = open(NAMESPACE_PATH, \"r\").read()\n",
    "\n",
    "config = {\n",
    "    \"MINIO_HOST_URL\": os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n",
    "    \"MINIO_ACCESS_KEY\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    \"MINIO_SECRET_KEY\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    \"KSERVE_MODEL_NAME\": \"fraud-detection\",\n",
    "    \"CURRENT_USER\": NAMESPACE,\n",
    "    \"NAMESPACE\": NAMESPACE,\n",
    "    \"BUCKET\": \"experiments\",\n",
    "    \"SOURCE_PATH\": \"dataset/feed.csv\",\n",
    "    \"SERVICE_ACCOUNT\": \"kserve-minio-sa\",\n",
    "    \"PROTOCOL_VERSION\": \"v2\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    endpoint_url=os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n",
    "    verify=False)\n",
    "\n",
    "bucket_name = config.get(\"BUCKET\")\n",
    "buckets = client.list_buckets()\n",
    "\n",
    "if not any(bucket['Name'] == bucket_name for bucket in buckets['Buckets']):\n",
    "    client.create_bucket(Bucket=config.get(\"BUCKET\"))\n",
    "\n",
    "if os.path.exists(\"dataset\"):\n",
    "    train_dataset = os.path.join(\"dataset\", \"feed.csv\")\n",
    "\n",
    "    client.upload_file(Filename=train_dataset, \n",
    "                       Bucket=config.get(\"BUCKET\"), \n",
    "                       Key=f\"{train_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Helper Functions**\n",
    "\n",
    "In this section, you define the function for plotting the Receiver Operating Characteristic Area Under the Curve\n",
    "(ROC_AUC). The ROC curve is a powerful diagnostic tool used for assessing the performance of binary classification\n",
    "models. It is a plot with the True Positive Rate (TPR) or sensitivity on the $y$-axis, and the False Positive Rate (FPR)\n",
    "or 1-specificity on the $x$-axis. Both rates range between `0` and `1`. Each point on the ROC curve represents a\n",
    "sensitivity/specificity pair corresponding to a particular decision threshold.\n",
    "\n",
    "The Area Under the ROC Curve (AUC-ROC) is a single scalar value that aggregates the performance of the classifier over\n",
    "all possible thresholds, providing a measure of the model's ability to distinguish between positive and negative\n",
    "classes. Here's a breakdown of what the AUC-ROC value means:\n",
    "\n",
    "- An AUC of `1.0` indicates that the classifier is perfect - it has a 100% true positive rate and a 0% false positive\n",
    "  rate.\n",
    "- An AUC of `0.5` suggests that the classifier is no better than random guessing - it has an equal chance of classifying\n",
    "  a positive sample as negative, and vice versa.\n",
    "- An AUC of less than `0.5` implies that the classifier is worse than random guessing - it's as if the model is\n",
    "  \"learning\" to make the wrong predictions.\n",
    "\n",
    "By visualizing the ROC curve, you can better understand the trade-off between sensitivity (the ability to correctly\n",
    "classify true positives) and specificity (the ability to correctly classify true negatives). This helps in selecting the\n",
    "optimal threshold that balances both metrics according to the specific needs of a given application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def init_minio_client():\n",
    "    client = boto3.client(\n",
    "        service_name=\"s3\",\n",
    "        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        endpoint_url=os.environ[\"MLFLOW_S3_ENDPOINT_URL\"],\n",
    "        verify=False)\n",
    "    return client\n",
    "\n",
    "\n",
    "def plot_roc_auc(y_test, preds):\n",
    "    \"\"\"Plot the Receiver Operating Characteristic (ROC) curve.\"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exploratory Data Analysis**\n",
    "\n",
    "In this section, you undertake a detailed Exploratory Data Analysis (EDA) of the dataset, aiming to uncover critical\n",
    "insights that could inform your subsequent analysis. The initial snapshot of the dataset, as seen below, reveals nine\n",
    "feature columns and a target column. The feature columns are as follows:\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:left\">Feature</th>\n",
    "    <th style=\"text-align:left\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Step</td>\n",
    "    <td style=\"text-align:left\">\n",
    "        Represents the simulation day. The simulation ran for a total of 180 steps, equivalent to six months.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Customer</td>\n",
    "    <td style=\"text-align:left\">Denotes the unique ID assigned to each customer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">zipCodeOrigin</td>\n",
    "    <td style=\"text-align:left\">Represents the originating or source zip code.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Merchant</td>\n",
    "    <td style=\"text-align:left\">Specifies the unique ID of the merchant.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">zipMerchant</td>\n",
    "    <td style=\"text-align:left\">Represents the zip code associated with the merchant.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Age</td>\n",
    "    <td style=\"text-align:left\">\n",
    "        Classifies the age of customers:\n",
    "        <ul>\n",
    "            <li>0: <= 18 years</li>\n",
    "            <li>1: 19-25 years</li>\n",
    "            <li>2: 26-35 years</li>\n",
    "            <li>3: 36-45 years</li>\n",
    "            <li>4: 46-55 years</li>\n",
    "            <li>5: 56-65 years</li>\n",
    "            <li>6: > 65 years</li>\n",
    "            <li>U: Unknown</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Gender</td>\n",
    "    <td style=\"text-align:left\">\n",
    "        Indicates the gender of the customer:\n",
    "        <ul>\n",
    "            <li>E : Enterprise</li>\n",
    "            <li>F: Female</li>\n",
    "            <li>M: Male</li>\n",
    "            <li>U: Unknown</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Category</td>\n",
    "    <td style=\"text-align:left\">Denotes the category of the purchase.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Amount</td>\n",
    "    <td style=\"text-align:left\">Specifies the purchase amount.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Fraud</td>\n",
    "    <td style=\"text-align:left\">The target variable, indicates whether the transaction was fraudulent (1) or benign (0)\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's fetch the dataset from our local S3 object storage and import it as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:fraud_detection_dataset",
     "prev:initialize"
    ]
   },
   "outputs": [],
   "source": [
    "file_name = config.get(\"SOURCE_PATH\")\n",
    "\n",
    "client = init_minio_client()\n",
    "client.download_file(\n",
    "    Bucket=config.get(\"BUCKET\"),\n",
    "    Key=config.get(\"SOURCE_PATH\"),\n",
    "    Filename=\"feed.csv\"\n",
    ")\n",
    "\n",
    "data = pd.read_csv(\"feed.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let's take a closer look at the types of data stored in each column and assess whether there are any missing\n",
    "values present in the dataset. When preparing data for machine learning modelling, this should **always** be considered a necessary step - regardless of the level of trust in a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, this dataset does not contain any missing values, meaning no imputation strategies are required in the preprocessing steps this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synthetic Minority Over-sampling TEchnique (SMOTE)\n",
    "\n",
    "Fraudulent data, as you can also observe from the plot below, tends to be imbalanced. This imbalance can lead to a bias\n",
    "towards the majority class during the training of a machine learning model. \n",
    "\n",
    "To combat this bias,\n",
    "we can use techniques such as oversampling or undersampling: \n",
    "\n",
    "- Oversampling refers to the process of augmenting the number of instances in the minority class by generating similar\n",
    "  instances.\n",
    "\n",
    "- Undersampling involves reducing the number of instances in the majority class by randomly selecting data points until\n",
    "  the count aligns with the minority class.\n",
    "\n",
    "Each of these strategies has associated risks. For instance, oversampling may result in the creation of duplicate or\n",
    "highly similar data points, which may not be beneficial for fraud detection given that fraudulent transactions often\n",
    "exhibit unique characteristics. Undersampling, however, implies a loss of data points, and consequently, valuable\n",
    "information.\n",
    "\n",
    "To balance the dataset without introducing excessive redundancy or losing crucial information, we employ an\n",
    "oversampling technique known as **SMOTE** (Synthetic Minority Over-sampling TEchnique). Unlike naive oversampling methods,\n",
    "SMOTE generates synthetic instances of the minority class using neighboring instances, ensuring that the new samples are\n",
    "not exact copies but closely resemble existing instances. This technique can potentially improve the performance of your\n",
    "model by providing it with a more representative and balanced view of the different classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our dataset to perform SMOTE. First, we split the dataset into fraudulent and not fraudlent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:data_preprocessing",
     "prev:fraud_detection_dataset"
    ]
   },
   "outputs": [],
   "source": [
    "# Create two dataframes with fraud and non-fraud data \n",
    "df_fraud = data.loc[data.fraud == 1] \n",
    "df_non_fraud = data.loc[data.fraud == 0]\n",
    "\n",
    "sns.countplot(x=\"fraud\",data=data)\n",
    "plt.title(\"Count of Fraudulent Payments\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of normal examples:\", df_non_fraud.fraud.count())\n",
    "print(\"Number of fradulent examples:\", df_fraud.fraud.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's examine the fraudlent data on its own to gain a better understanding of the patterns\n",
    "and behaviors of fraudsters. We can then use this information to improve the effectiveness of the detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Mean feature values per category:\")\n",
    "data.groupby('category')[['amount', 'fraud']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes apparent in the data that the 'leisure' and 'travel' categories seem to be most\n",
    "frequently targeted by fraudsters. Could it be that perpetrators might be strategically choosing categories where\n",
    "people typically spend more on average?\n",
    "\n",
    "To validate this hypothesis, we need to delve deeper into the transaction data, specifically comparing the amounts\n",
    "transacted in fraudulent and non-fraudulent cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Create two dataframes with fraud and non-fraud data \n",
    "pd.concat([df_fraud.groupby('category')['amount'].mean(),\n",
    "           df_non_fraud.groupby('category')['amount'].mean(),\n",
    "           data.groupby('category')['fraud'].mean()*100],\n",
    "           keys=[\"Fraudulent\",\"Non-Fraudulent\",\"Percent(%)\"], axis=1,sort=False).sort_values(by=['Non-Fraudulent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The initial hypothesis that fraudsters predominantly target categories where average spending is\n",
    "higher, only holds true to an extent. However, a clear trend emerges when you examine the transaction values\n",
    "associated with fraudulent activities.\n",
    "\n",
    "As illustrated in the table, you can confidently assert that a fraudulent transaction is typically significantly\n",
    "larger — about four times or more — than the average transaction within a given category. This significant deviation in\n",
    "transaction amounts may provide a useful indicator when identifying potential fraudulent activities in the future.\n",
    "\n",
    "Next, let's examine the average spend amount across the various catagories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot histograms of the amounts in fraud and non-fraud data \n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.boxplot(x=data.category, y=data.amount)\n",
    "plt.title(\"Boxplot for the Amount spend in category\")\n",
    "plt.ylim(0, 4000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We observe that spending is typically similar across all catagories, except the 'travel' category - with spending reaching significantly higher levels.\n",
    "\n",
    "This deviation in the 'travel' category could be due to a variety of factors, such as the inherent high cost associated\n",
    "with travel and tourism activities. Such information is crucial, not only for understanding the spending behavior of\n",
    "customers but also for improving the fraud detection strategies, as categories with higher average spending might\n",
    "attract more fraudulent activities.\n",
    "\n",
    "To reinforce these observations, we generate a histogram to present the relationship between\n",
    "the number and amount of fraudulent transactions. While the count of fraudulent transactions is relatively low, the\n",
    "monetary value they represent is disproportionately high. This pattern underscores the serious financial implications of\n",
    "fraud, even when the number of incidents might appear relatively minor at first glance. It's precisely this disparity\n",
    "that makes effective and precise fraud detection systems crucial for the integrity of any financial system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot histograms of the amounts in fraud and non-fraud data \n",
    "plt.hist(df_fraud.amount, alpha=0.5, label='fraud',bins=100)\n",
    "plt.hist(df_non_fraud.amount, alpha=0.5, label='nonfraud',bins=100)\n",
    "plt.title(\"Histogram for fraudulent and nonfraudulent payments\")\n",
    "plt.ylim(0, 10000)\n",
    "plt.xlim(0, 1000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Examining the table below, you can observe the percentage of fraudulent transactions within each age category. Among\n",
    "known age categories, the group '0' (representing ages 18 and under) exhibits the highest fraud percent, standing at\n",
    "`1.957586`. This data is crucial for enhancing the understanding of the demographics most vulnerable to fraudulent\n",
    "activities and can be instrumental in tailoring the fraud detection algorithms and preventive measures accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "((data.groupby('age')['fraud'].mean() * 100).reset_index()\n",
    "                                            .rename(\n",
    "                                                columns={\n",
    "                                                    'age': 'Age',\n",
    "                                                    'fraud': 'Fraud Percent'})\n",
    "                                            .sort_values(by='Fraud Percent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Data Preprocessing**\n",
    "\n",
    "In this section, you focus on preprocessing the data and preparing it for the training phase. Upon investigating the\n",
    "data, you can see that there are two columns with only one unique zip code value. In terms of ML, a feature with a\n",
    "single value adds no predictive power, since it remains constant for all observations. Therefore, you can drop this\n",
    "column from the dataset to streamline the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Unique 'zipCodeOri' values:\", data.zipcodeOri.nunique())\n",
    "print(\"Unique 'zipMerchant' values:\", data.zipMerchant.nunique())\n",
    "# dropping zipcodeori and zipMerchant since they have only one unique value\n",
    "data_reduced = data.drop([\"zipcodeOri\", \"zipMerchant\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, you convert the categorical features into numerical values. One efficient way to transform categorical values into\n",
    "numerical representations is by utilizing the pandas library's `cat.codes` property. This method allows you to encode\n",
    "categorical variables into numerical codes without significantly increasing the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turning object columns type to categorical for easing the transformation process\n",
    "col_categorical = data_reduced.select_dtypes(include= ['object']).columns\n",
    "for col in col_categorical:\n",
    "    data_reduced[col] = data_reduced[col].astype('category')\n",
    "# categorical values ==> numeric values\n",
    "data_reduced[col_categorical] = data_reduced[col_categorical].apply(lambda x: x.cat.codes)\n",
    "data_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To proceed with model training, you need to define the independent variable ($X$) and dependent/target variable ($y$).\n",
    "In this context, the independent variable ($X$) refers to the set of features or attributes that you use to predict the\n",
    "dependent/target variable ($y$). By properly defining $X$ and $y$, you can establish the foundation for training your ML\n",
    "model and exploring the relationships between the independent variables and the dependent/target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "X = data_reduced.drop(['fraud'], axis=1)\n",
    "y = data['fraud']\n",
    "\n",
    "print(X.head(), \"\\n\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Oversampling with SMOTE\n",
    "\n",
    "To address the issue of class imbalance in the dataset, you apply the Synthetic Minority Over-sampling TEchnique\n",
    "(SMOTE). As you saw earlier, this oversampling technique generates synthetic instances of the minority class (fraudulent\n",
    "transactions) by interpolating between existing instances. By applying SMOTE, you effectively increase the number of\n",
    "instances in the minority class to match the number of instances in the majority class (non-fraudulent transactions). As\n",
    "a result, you have an equal number of instances for both classes, which helps to alleviate the potential bias and\n",
    "improve the performance of your ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:oversampling_with_smote",
     "prev:data_preprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "y_res = pd.DataFrame(y_res)\n",
    "\n",
    "print(y_res.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train-Test Split for Model Performance Measurement\n",
    "\n",
    "To assess the performance of your ML model, you split the data into two sets: a training set and a testing set. The\n",
    "training set will be used to train the model, allowing it to learn patterns and relationships within the data. The\n",
    "testing set, on the other hand, will be used to evaluate the model's performance on unseen data. By measuring the\n",
    "model's performance on the testing set, you can gain insights into how well it generalizes to new and unseen instances.\n",
    "\n",
    "While cross-validation is a commonly recommended practice for model evaluation, in this case, due to the large number of\n",
    "instances in the dataset, you could opt for a simple train-test split. However, it is important to note that\n",
    "cross-validation should be used whenever feasible, as it provides a more comprehensive evaluation of the model's\n",
    "performance and helps to mitigate potential biases introduced by a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As mentioned earlier, fraud datasets often suffer from severe class imbalance, where the majority of instances are\n",
    "non-fraudulent transactions. If you were to naively predict non-fraudulent for all instances in such imbalanced\n",
    "datasets, you would achieve a high accuracy score, typically around 99%. However, this misleading accuracy score does\n",
    "not indicate a successful fraud detection system. In reality, the goal of a fraud detection classifier is to identify\n",
    "the fraudulent transactions accurately, which are the minority class in the dataset.\n",
    "\n",
    "To accurately evaluate the performance of a fraud detection classifier, it is essential to consider metrics that are\n",
    "sensitive to both the minority (fraudulent) and majority (non-fraudulent) classes. Metrics such as precision, recall,\n",
    "F1-score, and the area under the Receiver Operating Characteristic (ROC) curve provide a more comprehensive assessment\n",
    "of the model's effectiveness in detecting fraud.\n",
    "\n",
    "In addition, it is important to establish a baseline accuracy score that exceeds the accuracy achieved by simply\n",
    "predicting the majority class (non-fraudulent). This baseline accuracy serves as a benchmark for evaluating the\n",
    "performance of the fraud detection model, ensuring that it performs significantly better than a simplistic approach and\n",
    "demonstrates its ability to accurately detect fraudulent transactions.\n",
    "\n",
    "Therefore, when evaluating the performance of a fraud detection model, it is imperative to consider multiple metrics\n",
    "that provide a comprehensive understanding of its effectiveness in detecting both fraudulent and non-fraudulent\n",
    "instances, rather than relying solely on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# The base score should be better than predicting always non-fraduelent\n",
    "print(\"Base accuracy score: \", \n",
    "      df_non_fraud.fraud.count() / np.add(df_non_fraud.fraud.count(),df_fraud.fraud.count()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**\n",
    "Now that we've prepared our data, let's train a prediction model using several machine learning techniques. Why more than one? You will see below. Additionally, learning about the most common machine learning techniques is objectively fun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## K-Neighbours Classifier\n",
    "\n",
    "The K-Nearest Neighbors (KNN) classifier is a popular ML algorithm used for classification. In this section, you train a\n",
    "KNN classifier as a potential approach for the fraud detection problem.\n",
    "\n",
    "The KNN classifier works based on the principle that instances with similar feature values tend to belong to the same\n",
    "class. It classifies new instances by finding the K nearest neighbors in the training set and assigning the majority\n",
    "class label among those neighbors to the new instance.\n",
    "\n",
    "Key features of the KNN classifier include:\n",
    "\n",
    "- K value: The value of K represents the number of neighbors to consider for classification. It is an important\n",
    "  parameter that needs to be carefully chosen to achieve optimal performance.\n",
    "- Distance metric: The choice of distance metric, such as Euclidean distance or Manhattan distance, determines the\n",
    "  similarity between instances and influences the classification process.\n",
    "- Computational cost: The KNN classifier can be computationally expensive, especially for large datasets, as it requires\n",
    "  calculating distances between the new instance and all training instances. Therefore, it is essential to consider the\n",
    "  computational trade-offs when working with KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:knn_classifier",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "knn.fit(X_train, np.ravel(y_train, order='C'))\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for K-Nearest Neighbours: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of K-Nearest Neigbours: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, knn.predict_proba(X_test)[:,1])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(knn, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/k-neighbors/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our KNN classifier is a **Confusion Matrix**, where the rows represent Predicted Positive/Negative and the columns represent Actual Positive/Negative. Here we see that the top left entry (Predicted Positive, Actual Positive) and the bottom right entry (Predicted Negative, Actual Negative) make up . The average ratio between the false positive and negatives is given by the **Accuracy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "The Random Forest classifier is a powerful and versatile ML algorithm widely used for classification. In the context of\n",
    "the fraud detection problem, let's explore the Random Forest classifier as a potential approach.\n",
    "\n",
    "The Random Forest algorithm is an ensemble method that works by constructing a multitude of decision trees and\n",
    "aggregating their predictions to make the final classification. Each decision tree in the Random Forest is trained on a\n",
    "different subset of the data, using a random selection of features. This randomness helps to reduce overfitting and\n",
    "improve the generalization ability of the model.\n",
    "\n",
    "Key features of the Random Forest classifier include:\n",
    "\n",
    "- Ensemble learning: The Random Forest classifier combines the predictions of multiple decision trees to make a more\n",
    "  robust and accurate prediction. The ensemble approach helps to mitigate the risk of individual decision trees making\n",
    "  errors.\n",
    "- Feature importance: The Random Forest classifier provides a measure of feature importance, indicating the relative\n",
    "  importance of each feature in making predictions. This information can be valuable for understanding the key factors\n",
    "  contributing to fraudulent transactions.\n",
    "- Parallelization: The Random Forest algorithm lends itself well to parallelization, as each decision tree in the forest\n",
    "  can be trained independently. This makes it suitable for large datasets and can lead to faster training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:random_forest_classifier",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=50,\n",
    "                                max_depth=8,\n",
    "                                random_state=42,\n",
    "                                verbose=1,\n",
    "                                class_weight=\"balanced\")\n",
    "rf_clf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for Random Forest Classifier: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of Random Forest Classifier: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, rf_clf.predict_proba(X_test)[:,1])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(rf_clf, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/random_forest/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The art of data science is to know that which techniques and tool to use for particular use cases. In this case, using a Random Forest Classifier on our dataset to predict fraudulent transactions would have resulted in less accurate predictions than using a KNN classifier. Whilst it may only be ~1%, across **trillions** of daily transactions, every percentage point of accuracy results in saving millions of dollars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost Classifier\n",
    "\n",
    "The XGBoost (Extreme Gradient Boosting) classifier is a state-of-the-art ML algorithm known for its exceptional\n",
    "performance and widespread use in various domains, including fraud detection. Let's delve into the XGBoost classifier\n",
    "and its relevance to the fraud detection problem.\n",
    "\n",
    "XGBoost is an ensemble learning method that combines the power of gradient boosting with several innovative techniques.\n",
    "It excels at handling large-scale datasets and effectively capturing complex relationships between features. The\n",
    "algorithm constructs a series of decision trees iteratively, where each subsequent tree corrects the mistakes made by\n",
    "the previous trees.\n",
    "\n",
    "Key features of the XGBoost classifier include:\n",
    "\n",
    "- Gradient boosting: XGBoost utilizes gradient boosting, a technique that sequentially adds decision trees to improve\n",
    "  the model's predictive accuracy. By iteratively minimizing a specified loss function, XGBoost focuses on capturing\n",
    "  intricate patterns and relationships in the data.\n",
    "- Feature importance: XGBoost provides valuable insights into feature importance by quantifying the impact of each\n",
    "  feature on the model's performance. This information aids in identifying the most influential features for detecting\n",
    "  fraudulent transactions.\n",
    "- Parallel processing: XGBoost supports parallel processing, enabling faster training times and efficient computation on\n",
    "  large-scale datasets. It leverages the capabilities of multicore processors and distributed computing frameworks for\n",
    "  accelerated model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:xgboost_classifier",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "XGBoost_CLF = xgb.XGBClassifier(silent=None, seed=42, colsample_bynode=1, max_depth=6, learning_rate=0.05, n_estimators=50, \n",
    "                                objective=\"binary:hinge\", booster='gbtree', missing=1,\n",
    "                                n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \n",
    "                                subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "                                base_score=0.5, random_state=42, verbosity=1)\n",
    "XGBoost_CLF.fit(X_train, y_train)\n",
    "y_pred = XGBoost_CLF.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for XGBoost: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, XGBoost_CLF.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(XGBoost_CLF, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/xgb/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression Classifier\n",
    "\n",
    "The Logistic Regression classifier is a well-established and widely used ML algorithm for binary classification tasks,\n",
    "making it relevant to the fraud detection problem. Let's explore the Logistic Regression classifier and its\n",
    "applicability to this scenario.\n",
    "\n",
    "Despite its name, Logistic Regression is a classification algorithm that models the probability of an instance belonging\n",
    "to a particular class. It is particularly suited for problems where the dependent variable is binary, as in this case\n",
    "where you aim to distinguish between fraudulent and non-fraudulent transactions.\n",
    "\n",
    "Key features of the Logistic Regression classifier include:\n",
    "\n",
    "- Probabilistic modeling: Logistic Regression models the relationship between the independent variables and the\n",
    "  probability of belonging to a specific class. It employs the logistic function (also known as the sigmoid function)\n",
    "  to map the output to a probability score.\n",
    "- Interpretability: Logistic Regression provides interpretable coefficients for each independent variable, which allows\n",
    "  us to understand the impact of the features on the likelihood of fraud. These coefficients indicate the direction and\n",
    "  magnitude of the relationship between each feature and the probability of fraudulent transactions.\n",
    "- Efficiency: Logistic Regression is computationally efficient and can handle large datasets with relative ease. It\n",
    "  converges quickly and is less prone to overfitting, making it suitable for situations where interpretability and\n",
    "  simplicity are important factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:logistic_regression_cls",
     "prev:oversampling_with_smote"
    ]
   },
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(max_iter=999, solver='lbfgs')\n",
    "LRmodel.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = LRmodel.predict(X_test)\n",
    "print(\"Classification Report for LogisticRegression: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix of LogisticRegression: \\n\", confusion_matrix(y_test, y_pred))\n",
    "plot_roc_auc(y_test, LRmodel.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# save the model to minio s3 object\n",
    "filename = \"model.pkl\"\n",
    "pickle.dump(LRmodel, open(f\"{filename}\", \"wb\"))\n",
    "\n",
    "minio_client = init_minio_client()\n",
    "minio_client.upload_file(Filename=filename, \n",
    "                         Bucket=config.get(\"BUCKET\"), \n",
    "                         Key=f\"banking/pickles/logisticregression/model/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging several techniques rather than simply relying on one, we can validate the best model better than had we just relied on the accuracy result. A model that can predict 92% may sound promising, but not when compared to the results of other techniques. Through this methodolgy, we have validated that the KNN classifier is the best. We will deploy this model to make our predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Model Deployment**\n",
    "\n",
    "After training and evaluating the best model for fraud detection, the next step is to deploy it into a production environment\n",
    "where it can be used to detect fraud in real-time transactions.\n",
    "\n",
    "Start by creating a secure environment for accessing the S3 endpoint. To this\n",
    "end, you create a ServiceAccount object, associating it with a secret to establish an identity for the deployment\n",
    "process.\n",
    "\n",
    "After which, we create a manifest to deploy the model using a Kserve InferenceService. KServe InferenceService (often abbreviated as isvc) is a Kubernetes custom resource defined by KServe, a serverless framework designed specifically for machine learning model serving. An InferenceService facilitates the deployment and scaling of machine learning models in a Kubernetes cluster, allowing for easy and efficient serving of these models via a Kubeflow Endpoint. We will call upon this endpoint to perform a prediction on a given transaction. \n",
    "\n",
    "By running Kubeflow on top of HPE Ezmeral Unified Analytics software, we can easily spin up and allocate the resources needed to this InferenceService right here from the notebook - no additional steps required! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:model_deployment",
     "prev:initialize",
     "prev:knn_classifier",
     "prev:random_forest_classifier",
     "prev:xgboost_classifier",
     "prev:logistic_regression_cls"
    ]
   },
   "outputs": [],
   "source": [
    "manifest = f\"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: s3creds\n",
    "  annotations:\n",
    "     serving.kserve.io/s3-endpoint: {os.environ[\"AWS_ENDPOINT_URL\"].replace(\"http://\", \"\")}\n",
    "     serving.kserve.io/s3-usehttps: \"0\"\n",
    "     serving.kserve.io/s3-useanoncredential: \"false\"\n",
    "     serving.kserve.io/s3-cabundle: \"\"\n",
    "type: Opaque\n",
    "stringData:\n",
    "  AWS_ACCESS_KEY_ID: {os.environ[\"AWS_ACCESS_KEY_ID\"]}\n",
    "  AWS_SECRET_ACCESS_KEY: {os.environ[\"AWS_SECRET_ACCESS_KEY\"]}\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: kserve-minio-sa\n",
    "secrets:\n",
    "- name: s3creds\n",
    "\n",
    "---\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"fraud-detection\"\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-minio-sa\n",
    "    sklearn:\n",
    "      protocolVersion: \"v2\"\n",
    "      resources:\n",
    "        requests:\n",
    "          cpu: 0.5\n",
    "        limits:\n",
    "          cpu: 0.5\n",
    "      storageUri: \"s3://{config['BUCKET']}/banking/pickles/k-neighbors/model\"\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(\"manifests\", exist_ok=True)\n",
    "\n",
    "with open(os.path.join(\"manifests\", \"isvc.yaml\"), \"w\") as f:\n",
    "    f.write(manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the InferenceService. This may take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = subprocess.run([\"kubectl\", \"apply\", \"-f\", \"manifests/isvc.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the status of our endpoint. Navigate to your HPE Ezmeral Unified Analytics software dashboard, open the sidebar navigation menu and under \"Data Science\", select \"Model Serving\". When a green tick appears next to \"fraud-detection\", the service is up and running! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Prediction**\n",
    "\n",
    "With the fraud detection model deployed as an endpoint, we can now use it to make predictions on new, incoming transactions. The\n",
    "prediction process involves passing the relevant transaction data through the deployed model to obtain a prediction of\n",
    "whether or not the transaction is fraudulent.\n",
    "\n",
    "For this tutorial, the new, incoming transactions are provided in the 'generated-data' - a dataset of transactions the model has not yet seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip",
     "prev:model_deploy"
    ]
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(\"dataset\", \"generated-data.csv\"))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all data, be it for training or inference, it must be prepared prior to being parsed through a model. We can see that this dataset comes with merchant, customer and catagory information is not in the same format as the training dataset. There are also additional fields relating to the zip codes of where the transaction took place, that our model would be unfamiliar with. \n",
    "\n",
    "Let's do a bit of pre-processing work to put this new data into a workable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Drop the zip code information, as it was not in the training dataset. \n",
    "data_reduced = data.drop(['zipcodeOri', 'zipMerchant'], axis=1)\n",
    "\n",
    "#Reformat the customer, merchant and catagory columns to match the format of the training dataset.  \n",
    "data_reduced.loc[:, ['customer', 'merchant', 'category']].astype('category')\n",
    "\n",
    "col_categorical = data_reduced.select_dtypes(include= ['object']).columns\n",
    "for col in col_categorical:\n",
    "    data_reduced[col] = data_reduced[col].astype('category')\n",
    "\n",
    "data_reduced[col_categorical] = data_reduced[col_categorical].apply(lambda x: x.cat.codes)\n",
    "data_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform some predictions. What's even better is that this new dataset comes with a 'fraud' flag, such that we can verify ourselves that our model/application is working as it should. \n",
    "\n",
    "To do this, we parse the Endpoint URL to make a request to it. This is made significantly easier thanks to HPE Ezmeral Unified Analytics software, where we can simply pop in our local cluster name and the variables we declared earlier that are unique to this use case. We ensure that the URL formatting matches what we declared in the manifest earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "DOMAIN_NAME = \"svc.cluster.local\"  #You change this to your domain for external access!\n",
    "NAMESPACE = config.get(\"NAMESPACE\")\n",
    "DEPLOYMENT_NAME = config.get(\"KSERVE_MODEL_NAME\")\n",
    "MODEL_NAME = DEPLOYMENT_NAME\n",
    "SVC = f'{DEPLOYMENT_NAME}-predictor-default.{NAMESPACE}.{DOMAIN_NAME}'\n",
    "URL = f\"https://{SVC}/v2/models/{MODEL_NAME}/infer\"\n",
    "\n",
    "print(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and there is our model, being served as an endpoint by Kserve! Now, any notebook or any other application within the HPE Ezmeral Unified Analytics software stack and make calls to our model on demand.\n",
    "\n",
    "Let's test a transaction. We first drop the 'fraud' column from the dataset to give the model a blind test. \n",
    "\n",
    "We know the fourth entry (3) in our test dataset is a fraudulent transaction, so we parse it in as the \"data\" parameter in our request message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Drop the fraudulent flag column\n",
    "X = data_reduced.drop(['fraud'], axis=1)\n",
    "y = data_reduced['fraud']\n",
    "print(\"There are \" + len(X.values) + \" test entries.\")\n",
    "\n",
    "#Create the request\n",
    "inference_request = {\n",
    "    \"inputs\" : [{\n",
    "        \"name\" : \"fraud-detection-infer-001\",\n",
    "        \"datatype\": \"FP32\",\n",
    "\n",
    "        \"shape\": [1, 7],\n",
    "        # Example of non-fraudulent Transaction Dtls\n",
    "        # \"data\": [list(item) for item in X.values][14],\n",
    "\n",
    "        # Example of a fraudulent request\n",
    "        \"data\": [list(item) for item in X.values][3],\n",
    "    }]\n",
    "}\n",
    "\n",
    "print(\"data:\", inference_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we send the request to the model endpoint to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Start the requests session\n",
    "session = requests.Session()\n",
    "message = {\"message\":\"\", \"value\":\"\"}\n",
    "\n",
    "#Add headers and POST our message\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ['AUTH_TOKEN']}\"}\n",
    "response = requests.post(URL, json=inference_request, headers=headers, verify=False)\n",
    "\n",
    "#If we get a response back, return the approproate message. \n",
    "if response.status_code == 200:\n",
    "\n",
    "    #If the output value from the model is \"1\", it was a fraudulent transaction! \n",
    "    if json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0] != None and json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0] == 1:\n",
    "        message['message'] = \"Fraudulent Banking Transaction!\"\n",
    "        message['value'] = json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0]        \n",
    "        print('\\033[91m' \"Prediction Result:\", json.dumps(message))\n",
    "    elif len(json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'])>1:\n",
    "        print(\"Model-Infer-dtl:[data]:\\n\", json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'])\n",
    "    else:\n",
    "        #If the output value from the model is \"0\", it was a not a fraudulent transaction.\n",
    "        message['message'] = \"Non-fraudulent Banking Transaction!\"\n",
    "        message['value'] = json.loads(response.__dict__.get('_content')).get('outputs')[0]['data'][0]   \n",
    "        print('\\033[92m'  \"Prediction Result:\", json.dumps(message))\n",
    "else:\n",
    "    print(response.status_code, response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Deploying a front-end application on HPE Ezmeral Unified Analytics**\n",
    "\n",
    "What if we want to test transactions outside of a notebook environment, be it by security teams in a bulk analysis tool, or an automated real-time monitoring system? Let's see what this might look like in a production environment!\n",
    "\n",
    "1. Go to the \"applications\" folder in this repository. \n",
    "1. Save the \"fraud-detection.tgz\" and \"icon.png\" locally by right-clicking on each and selecting \"Download\".\n",
    "1. Navigate back to your HPE Ezmeral Unified Analytics dashboard.\n",
    "1. In the sidebar navigation menu of your dashboard, select \"Tools and Frameworks\".\n",
    "1. Select \"Import Framework\" in the top right hand corner. \n",
    "1. Give your application a name, set the Version to 1.0.0 and provide a brief description of what this application is (e.g. \"Fraudulent Transaction Detector\") \n",
    "1. Set the Catagory to \"Analytics\" and drag the icon from the \"application\" folder of this repository into the \"Framework Icon\" field. Click \"Framework Chart\" in the bottom right corner to continue.\n",
    "1. Drag the Helm Package (fraud-detection.tgz) from the \"application\" folder of this repository in to the \"Helm Package\" field, and provide a namespace (e.g. \"fraud-detection-app\"). Click \"Framework Values\".\n",
    "1. Check the manifest. No changes should be required. Click \"Review\".\n",
    "1. Click \"Submit\". \n",
    "\n",
    "You have successfully imported an application using Helm into HPE Ezmeral Unified Analytics software. This application will automatically provision itself and will be ready to launch in a few moments. To launch the application, simply click \"Open\" on your newly-created application under the \"Analytics\" tab in the \"Tools and Frameworks\" pane.\n",
    "\n",
    "In this application, you can:\n",
    "- Create new transactions and see if they would be triggered as fraudlent by the model.\n",
    "- Upload a CSV of transactions to test several transactions at once! Navigate to the \"dataset\" folder in this repository, right click on \"generated-data.csv\", click \"Download\", then drag the downloaded file into the \"Upload CSV\" field in the application. \n",
    "\n",
    "# **Conclusion**\n",
    "\n",
    "In this notebook, you trained, tested and deployed a model that can accurately detect fraudulent banking transactions. You employed\n",
    "various classifiers to determine the best method and achieve the best detection results. As fraud datasets often suffer\n",
    "from class imbalance, you utilized the SMOTE oversampling technique to address this issue by generating synthetic\n",
    "minority class examples. You observed how easy a data science workflow can be by having the data, tools and resources all managed for you by HPE Ezmeral Unified Analytics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "",
   "experiment": {
    "id": "bdd12faf-cf17-44ab-9df9-f12d162cac08",
    "name": "stage-experiment"
   },
   "experiment_name": "stage-experiment",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "fraud-detection-exp-14-06-2023-02",
   "pipeline_name": "fraud-detection-exp-14-06-2023-02",
   "snapshot_volumes": false,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:add-external-df-volume:true",
    "label:add-ldapcert-secret:true",
    "label:add-sssd-secret:true",
    "label:add-user-s3-secret:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "836de0bc6c3ec877b14e515fee0e932bbf60b1fe66c7ecc90fa579a75883c3a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
