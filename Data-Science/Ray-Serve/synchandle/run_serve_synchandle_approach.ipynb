{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4c69c53-5bdc-4c38-8864-b6694a0a2553",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Maximizing Online Inference: Exploring Ray Serve and Diverse Methodologies\n",
    "\n",
    "Ray Serve is an adaptable model deployment framework designed for constructing real-time inference APIs. It's framework-agnostic, allowing you to use a single toolkit to serve a wide range of tools,models and services."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b6eb40-1d5e-4156-8171-a482620095d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Serve Positioning](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/serve_positioning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b1d75f4-1ada-48d2-9c78-c3fffa47e1fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Navigating Ray Serve through RayServeSyncHandle's Methodological Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17538457-f7f1-41c7-a440-247d4cbb13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 22:40:18,353\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_d1bbb4a7b4b1de39.zip.\n",
      "2024-01-12 22:40:18,355\tINFO packaging.py:518 -- Creating a file package for local directory './'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_client_ray_version': '2.7.0', '_address': 'http://kuberay-head-svc.kuberay.svc.cluster.local:8265', '_cookies': None, '_default_metadata': {}, '_headers': None, '_verify': True, '_ssl_context': None}\n",
      "Ray job submitted with job_id: raysubmit_QGnr4Qj7Q93Fuzzd\n",
      "Ray job status for job_id raysubmit_QGnr4Qj7Q93Fuzzd: PENDING\n",
      "Ray job logs for job_id raysubmit_QGnr4Qj7Q93Fuzzd: \n",
      "Ray job info for job_id raysubmit_QGnr4Qj7Q93Fuzzd: type=<JobType.SUBMISSION: 'SUBMISSION'> job_id=None submission_id='raysubmit_QGnr4Qj7Q93Fuzzd' driver_info=None status=<JobStatus.PENDING: 'PENDING'> entrypoint='python snycheck.py' message='Job has not started yet. It may be waiting for resources (CPUs, GPUs, custom resources) to become available. It may be waiting for the runtime environment to be set up.' error_type=None start_time=1705099226532 end_time=None metadata={} runtime_env={'working_dir': 'gcs://_ray_pkg_d1bbb4a7b4b1de39.zip', 'pip': {'packages': ['textblob'], 'pip_check': False}, 'env_vars': {'http_proxy': 'http://10.78.90.46:80', 'https_proxy': 'http://10.78.90.46:80'}, '_ray_commit': 'b4bba4717f5ba04ee25580fe8f88eed63ef0c5dc'} driver_agent_http_address=None driver_node_id=None\n",
      "2024-01-12 14:41:38,419\tINFO worker.py:1329 -- Using address kuberay-head-svc.kuberay.svc.cluster.local:6379 set in the environment variable RAY_ADDRESS\n",
      "2024-01-12 14:41:38,419\tINFO worker.py:1458 -- Connecting to existing Ray cluster at address: kuberay-head-svc.kuberay.svc.cluster.local:6379...\n",
      "2024-01-12 14:41:38,533\tINFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.224.105.197:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=347, ip=10.224.105.197)\u001b[0m INFO 2024-01-12 14:42:25,212 http_proxy 10.224.105.197 http_proxy.py:1428 - Proxy actor f91931a330c3054cf56a933c02000000 starting on node fc309c118aba26f569f7ba411dbaae25f86a43ef96156f43a4332fd4.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=347, ip=10.224.105.197)\u001b[0m INFO 2024-01-12 14:42:25,224 http_proxy 10.224.105.197 http_proxy.py:1612 - Starting HTTP server on node: fc309c118aba26f569f7ba411dbaae25f86a43ef96156f43a4332fd4 listening on port 9000\n",
      "The new client HTTP config differs from the existing one in the following fields: ['host', 'port', 'location']. The new HTTP config is ignored.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=347, ip=10.224.105.197)\u001b[0m INFO:     Started server process [347]\n",
      "\u001b[2m\u001b[36m(ServeController pid=296, ip=10.224.105.197)\u001b[0m INFO 2024-01-12 14:42:25,294 controller 296 deployment_state.py:1390 - Deploying new version of deployment RayServeSyncHandleDeployment in application 'default'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=296, ip=10.224.105.197)\u001b[0m INFO 2024-01-12 14:42:25,397 controller 296 deployment_state.py:1679 - Adding 2 replicas to deployment RayServeSyncHandleDeployment in application 'default'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "import time\n",
    "from ray.runtime_env import RuntimeEnv\n",
    "\n",
    "# Ray cluster information\n",
    "ray_head_ip = \"kuberay-head-svc.kuberay.svc.cluster.local\"\n",
    "ray_head_port = 8265\n",
    "ray_address = f\"http://{ray_head_ip}:{ray_head_port}\"\n",
    "\n",
    "# Submit Ray job using JobSubmissionClient\n",
    "client = JobSubmissionClient(ray_address)\n",
    "job_id = client.submit_job(\n",
    "    entrypoint=\"python snycheck.py\",\n",
    "    runtime_env={\n",
    "        \"working_dir\": \"./\",\n",
    "        \"pip\": [\"textblob\"],\n",
    "        \"env_vars\": {\"http_proxy\":\"http://<proxy_host>:<port>\",\"https_proxy\":\"http://<proxy_host>:<port>\"}\n",
    "    },\n",
    "    entrypoint_num_cpus=1\n",
    ")\n",
    "\n",
    "print(client.__dict__)\n",
    "print(f\"Ray job submitted with job_id: {job_id}\")\n",
    "\n",
    "# Wait for a while to let the jobs run\n",
    "time.sleep(1)\n",
    "\n",
    "job_status = client.get_job_status(job_id)\n",
    "get_job_logs = client.get_job_logs(job_id)\n",
    "get_job_info = client.get_job_info(job_id)\n",
    "print(f\"Ray job status for job_id {job_id}: {job_status}\")\n",
    "print(f\"Ray job logs for job_id {job_id}: {get_job_logs}\")\n",
    "print(f\"Ray job info for job_id {job_id}: {get_job_info}\")\n",
    "async for lines in client.tail_job_logs(job_id):\n",
    "    print(lines, end=\"\") \n",
    "\n",
    "# Disconnect from the Ray cluster\n",
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49ac6078-88e2-4b7c-93be-52d61a186d9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "![NLP API Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/nlp_api_arch.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8016db4e",
   "metadata": {},
   "source": [
    "# The following code will reads the loggs and offers the right host node ip and port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lines, end=\"\") \n",
    "import re\n",
    "pattern = r\"http_proxy (\\d+\\.\\d+\\.\\d+\\.\\d+).*?port (\\d+)\"\n",
    "matches = re.findall(pattern, lines)\n",
    "\n",
    "# Display the extracted values\n",
    "for match in matches:\n",
    "    print(f\"http_proxy: {match[0]}, port: {match[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d93a41-ae8a-4e5a-8965-4f7360cc469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "The Blob has long captivated my imagination, emerging as the quintessential cinematic nightmare:\n",
    "an insatiable, amoebic entity with the eerie ability to breach virtually any defense, \n",
    "ominously described by a fated scientist as  assimilating flesh on contact. \n",
    "Mocking parallels to gelatin are futile for this concept embodies the gravest of implications, akin to the cataclysmic \n",
    "gray goo scenario envisioned by technophiles haunted by the specter of runaway artificial intelligence '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42d3b16-b2a2-4810-9301-a93c035c1be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_content': b'', '_content_consumed': True, '_next': None, 'status_code': 200, 'headers': {'date': 'Fri, 12 Jan 2024 22:44:04 GMT', 'server': 'envoy', 'content-type': 'text/plain', 'ray_serve_request_id': '3a9dc65a-ce3e-4ba8-a62b-da9b1fbcba03', 'x-request-id': '3a9dc65a-ce3e-4ba8-a62b-da9b1fbcba03', 'x-envoy-upstream-service-time': '5324', 'transfer-encoding': 'chunked'}, 'raw': <urllib3.response.HTTPResponse object at 0x7f27f48a4d60>, 'url': 'http://10.224.105.197:9000/check?input_text=%0AThe+Blob+has+long+captivated+my+imagination%2C+emerging+as+the+quintessential+cinematic+nightmare%3A%0Aan+insatiable%2C+amoebic+entity+with+the+eerie+ability+to+breach+virtually+any+defense%2C+%0Aominously+described+by+a+fated+scientist+as++assimilating+flesh+on+contact.+%0AMocking+parallels+to+gelatin+are+futile+for+this+concept+embodies+the+gravest+of+implications%2C+akin+to+the+cataclysmic+%0Agray+goo+scenario+envisioned+by+technophiles+haunted+by+the+specter+of+runaway+artificial+intelligence+', 'encoding': 'ISO-8859-1', 'history': [], 'reason': 'OK', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(seconds=5, microseconds=339893), 'request': <PreparedRequest [GET]>, 'connection': <requests.adapters.HTTPAdapter object at 0x7f280ce87c10>}\n",
      "200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(f\"http://{match[0]}:{match[1]}/check\", params={\"input_text\": input_text}).__dict__)\n",
    "print(requests.get(f\"http://{match[0]}:{match[1]}/check\", params={\"input_text\": input_text}).__dict__.get('status_code'))\n",
    "print(requests.get(f\"http://{match[0]}:{match[1]}/check\", params={\"input_text\": input_text}).__dict__.get('_content').decode('utf8'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd3fca75-4461-4b79-a511-df2f9ec6c40a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## conclusion\n",
    "\n",
    "Ray Serve is an adaptable model deployment framework designed for constructing real-time inference APIs. It's framework-agnostic, allowing you to use a single toolkit to serve a wide range of models, including deep learning models created with popular frameworks like PyTorch, TensorFlow, and Keras, as well as Scikit-Learn models and custom Python business logic. This versatile tool boasts an array of features and performance enhancements, such as response streaming, dynamic request batching, and multi-node/multi-GPU support, making it well-suited for handling Large Language Models and other demanding tasks.\n",
    "What sets Ray Serve apart is its proficiency in orchestrating the composition of multiple machine learning models and business logic components within a single Python-based inference service. Leveraging the power of Ray, it seamlessly scales across multiple machines and offers flexible scheduling capabilities, including fractional GPU allocation, which optimisation resource sharing and enables cost-effective deployment of a multitude of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e391cb9-9d83-4690-9f2c-1704bf0b290c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
