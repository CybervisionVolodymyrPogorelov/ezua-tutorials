{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d67c80-ee5a-42d5-8790-58b2017bcbb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Maximizing Online Inference: Exploring Ray Serve and Diverse Methodologies\n",
    "\n",
    "Ray Serve is an adaptable model deployment framework designed for constructing real-time inference APIs. It's framework-agnostic, allowing you to use a single toolkit to serve a wide range of tools,models and services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d1359-9327-4a31-bd4b-133e406633a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Serve Positioning](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/serve_positioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b47bbe-a47d-4180-9db5-00e0d0d26f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Serve Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/serve_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cad705c-f9e6-47b4-a5eb-900c06b8daee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/envs/ray/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/envs/ray/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/envs/ray/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/ray/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/ray/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/ray/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd15b52-e4c4-4ee6-a635-1c7049b05bff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A Deep Dive into Ray Serve with the Agile FastAPI Web Framework Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6374e0-8c6d-40c1-9c89-306fec2d2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 23:15:06,608\tINFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_7ea98a73ed9e687a.zip.\n",
      "2024-01-12 23:15:06,610\tINFO packaging.py:518 -- Creating a file package for local directory './'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_client_ray_version': '2.7.0', '_address': 'http://kuberay-head-svc.kuberay.svc.cluster.local:8265', '_cookies': None, '_default_metadata': {}, '_headers': None, '_verify': True, '_ssl_context': None}\n",
      "Ray job submitted with job_id: raysubmit_kVjDRGCj22F6mvNa\n",
      "Ray job status for job_id raysubmit_kVjDRGCj22F6mvNa: PENDING\n",
      "Ray job logs for job_id raysubmit_kVjDRGCj22F6mvNa: \n",
      "Ray job info for job_id raysubmit_kVjDRGCj22F6mvNa: type=<JobType.SUBMISSION: 'SUBMISSION'> job_id=None submission_id='raysubmit_kVjDRGCj22F6mvNa' driver_info=None status=<JobStatus.PENDING: 'PENDING'> entrypoint='python check.py' message='Job has not started yet. It may be waiting for resources (CPUs, GPUs, custom resources) to become available. It may be waiting for the runtime environment to be set up.' error_type=None start_time=1705101313538 end_time=None metadata={} runtime_env={'working_dir': 'gcs://_ray_pkg_7ea98a73ed9e687a.zip', 'pip': {'packages': ['textblob'], 'pip_check': False}, 'env_vars': {'http_proxy': 'http://10.78.90.46:80', 'https_proxy': 'http://10.78.90.46:80'}, '_ray_commit': 'b4bba4717f5ba04ee25580fe8f88eed63ef0c5dc'} driver_agent_http_address=None driver_node_id=None\n",
      "2024-01-12 15:16:25,891\tINFO worker.py:1329 -- Using address kuberay-head-svc.kuberay.svc.cluster.local:6379 set in the environment variable RAY_ADDRESS\n",
      "2024-01-12 15:16:25,892\tINFO worker.py:1458 -- Connecting to existing Ray cluster at address: kuberay-head-svc.kuberay.svc.cluster.local:6379...\n",
      "2024-01-12 15:16:25,997\tINFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttp://10.224.105.180:8265 \u001b[39m\u001b[22m\n",
      "The new client HTTP config differs from the existing one in the following fields: ['host', 'location']. The new HTTP config is ignored.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=469, ip=10.224.105.180)\u001b[0m INFO 2024-01-12 15:17:14,439 http_proxy 10.224.105.180 http_proxy.py:1428 - Proxy actor 1eb4863f79282e7c00cf917e04000000 starting on node 133cfac4a7f41cb932e513e7e649cc8995e49ac405052b9721df197d.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=469, ip=10.224.105.180)\u001b[0m INFO 2024-01-12 15:17:14,451 http_proxy 10.224.105.180 http_proxy.py:1612 - Starting HTTP server on node: 133cfac4a7f41cb932e513e7e649cc8995e49ac405052b9721df197d listening on port 8000\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=469, ip=10.224.105.180)\u001b[0m INFO:     Started server process [469]\n",
      "\u001b[2m\u001b[36m(ServeController pid=418, ip=10.224.105.180)\u001b[0m INFO 2024-01-12 15:17:14,523 controller 418 deployment_state.py:1390 - Deploying new version of deployment RAYFastAPIDeployment in application 'default'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=418, ip=10.224.105.180)\u001b[0m INFO 2024-01-12 15:17:14,626 controller 418 deployment_state.py:1679 - Adding 2 replicas to deployment RAYFastAPIDeployment in application 'default'.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.job_submission import JobSubmissionClient\n",
    "import time\n",
    "from ray.runtime_env import RuntimeEnv\n",
    "\n",
    "# Ray cluster information\n",
    "ray_head_ip = \"kuberay-head-svc.kuberay.svc.cluster.local\"\n",
    "ray_head_port = 8265\n",
    "ray_address = f\"http://{ray_head_ip}:{ray_head_port}\"\n",
    "\n",
    "# Submit Ray job using JobSubmissionClient\n",
    "client = JobSubmissionClient(ray_address)\n",
    "job_id = client.submit_job(\n",
    "    entrypoint=\"python check.py\",\n",
    "    runtime_env={\n",
    "        \"working_dir\": \"./\",\n",
    "        \"pip\": [\"textblob\"],\n",
    "        \"env_vars\": {\"http_proxy\":\"http://<proxy_host>:<port>\",\"https_proxy\":\"http://<proxy_host>:<port>\"}\n",
    "    },\n",
    "    entrypoint_num_cpus=1\n",
    ")\n",
    "\n",
    "print(client.__dict__)\n",
    "print(f\"Ray job submitted with job_id: {job_id}\")\n",
    "\n",
    "# Wait for a while to let the jobs run\n",
    "time.sleep(1)\n",
    "\n",
    "job_status = client.get_job_status(job_id)\n",
    "get_job_logs = client.get_job_logs(job_id)\n",
    "get_job_info = client.get_job_info(job_id)\n",
    "print(f\"Ray job status for job_id {job_id}: {job_status}\")\n",
    "print(f\"Ray job logs for job_id {job_id}: {get_job_logs}\")\n",
    "print(f\"Ray job info for job_id {job_id}: {get_job_info}\")\n",
    "async for lines in client.tail_job_logs(job_id):\n",
    "    print(lines, end=\"\") \n",
    "\n",
    "# Disconnect from the Ray cluster\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d3d2ad-3c73-4a5e-b936-716c358a19ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http_proxy: 10.224.105.180, port: 8000\n"
     ]
    }
   ],
   "source": [
    "# print(lines, end=\"\") \n",
    "import re\n",
    "pattern = r\"http_proxy (\\d+\\.\\d+\\.\\d+\\.\\d+).*?port (\\d+)\"\n",
    "matches = re.findall(pattern, lines)\n",
    "\n",
    "# Display the extracted values\n",
    "for match in matches:\n",
    "    print(f\"http_proxy: {match[0]}, port: {match[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a2a0c3-2808-4b40-85d8-27f97dfb5579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = '''\n",
    "The Blob has long captivated my imagination, emerging as the quintessential cinematic nightmare:\n",
    "an insatiable, amoebic entity with the eerie ability to breach virtually any defense, \n",
    "ominously described by a fated scientist as  assimilating flesh on contact. \n",
    "Mocking parallels to gelatin are futile for this concept embodies the gravest of implications, akin to the cataclysmic \n",
    "gray goo scenario envisioned by technophiles haunted by the specter of runaway artificial intelligence '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee8926-c634-4a01-82e5-ed7159da1345",
   "metadata": {
    "tags": []
   },
   "source": [
    "![NLP API Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_08/nlp_api_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9acd439a-d22c-4620-8ef5-05b9c09c480b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_content': b'', '_content_consumed': True, '_next': None, 'status_code': 200, 'headers': {'date': 'Fri, 12 Jan 2024 23:27:05 GMT', 'server': 'uvicorn', 'content-type': 'text/plain', 'ray_serve_request_id': '3be77e00-daa5-46b9-a067-02c70b58a364', 'x-request-id': '3be77e00-daa5-46b9-a067-02c70b58a364', 'Transfer-Encoding': 'chunked'}, 'raw': <urllib3.response.HTTPResponse object at 0x7f12dccdc880>, 'url': 'http://10.224.105.180:8000/check?input_text=%0AThe+Blob+has+long+captivated+my+imagination%2C+emerging+as+the+quintessential+cinematic+nightmare%3A%0Aan+insatiable%2C+amoebic+entity+with+the+eerie+ability+to+breach+virtually+any+defense%2C+%0Aominously+described+by+a+fated+scientist+as++assimilating+flesh+on+contact.+%0AMocking+parallels+to+gelatin+are+futile+for+this+concept+embodies+the+gravest+of+implications%2C+akin+to+the+cataclysmic+%0Agray+goo+scenario+envisioned+by+technophiles+haunted+by+the+specter+of+runaway+artificial+intelligence+', 'encoding': 'ISO-8859-1', 'history': [], 'reason': 'OK', 'cookies': <RequestsCookieJar[]>, 'elapsed': datetime.timedelta(microseconds=95794), 'request': <PreparedRequest [GET]>, 'connection': <requests.adapters.HTTPAdapter object at 0x7f12dccdcd30>}\n",
      "200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(f\"http://{match[0]}:{match[1]}/check\", params={\"input_text\": input_text}).__dict__)\n",
    "print(requests.get(f\"http://{match[0]}:{match[1]}/check\", params={\"input_text\": input_text}).__dict__.get('status_code'))\n",
    "print(requests.get(f\"http://{match[0]}:{match[1]}/check\", params={\"input_text\": input_text}).__dict__.get('_content').decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ed0f5-901a-4e7b-a8bd-6ab161db3961",
   "metadata": {
    "tags": []
   },
   "source": [
    "## conclusion\n",
    "\n",
    "Ray Serve is an adaptable model deployment framework designed for constructing real-time inference APIs. It's framework-agnostic, allowing you to use a single toolkit to serve a wide range of models, including deep learning models created with popular frameworks like PyTorch, TensorFlow, and Keras, as well as Scikit-Learn models and custom Python business logic. This versatile tool boasts an array of features and performance enhancements, such as response streaming, dynamic request batching, and multi-node/multi-GPU support, making it well-suited for handling Large Language Models and other demanding tasks.\n",
    "What sets Ray Serve apart is its proficiency in orchestrating the composition of multiple machine learning models and business logic components within a single Python-based inference service. Leveraging the power of Ray, it seamlessly scales across multiple machines and offers flexible scheduling capabilities, including fractional GPU allocation, which optimisation resource sharing and enables cost-effective deployment of a multitude of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35027aa2-4ff5-488e-b156-6623ec709e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ray",
   "language": "python",
   "name": "ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
